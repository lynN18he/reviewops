"""
ReviewOps Agentic Workflow - åŸºäº LangGraph çš„è‡ªåŠ¨åŒ–å·¡æ£€ç³»ç»Ÿ
"""

import os
import time
import random
from typing import TypedDict, List, Literal
from operator import add
from dotenv import load_dotenv

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.chat_models import ChatTongyi
import json

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ==================== Mock æ•°æ®æ±  ====================
# ä¼˜åŒ–åçš„ Mock æ•°æ®ï¼Œæ›´ç¬¦åˆ RAG åœºæ™¯
# åŒ…å«æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§è¯„è®ºï¼Œä¾¿äºæµ‹è¯•å„ç§åœºæ™¯
MOCK_DATA_POOL = {
    # è´Ÿé¢è¯„è®ºæ± ï¼ˆrating 1-2ï¼‰
    "negative": [
        # æ¡ˆä¾‹ 1ï¼šäº§å“ç¼ºé™· - ç”µæ± ç»­èˆªè™šæ ‡
        {
            "base_id": 101,
            "user_id": "user_001",
            "review_text": "æ ‡ç§°ç»­èˆª45åˆ†é’Ÿï¼Œå®é™…åªèƒ½é£20å¤šåˆ†é’Ÿï¼Œç»­èˆªä¸¥é‡è™šæ ‡ï¼Œæ„Ÿè§‰è¢«æ¬ºéª—äº†ã€‚å¤šæ¬¡æµ‹è¯•éƒ½æ˜¯è¿™æ ·ï¼Œæ˜æ˜¾æ˜¯äº§å“å‚æ•°é€ å‡ã€‚",
            "rating": 1
        },
        # æ¡ˆä¾‹ 2ï¼šäº§å“ç¼ºé™· - äº‘å°å¼€æœºè‡ªæ£€å¤±è´¥
        {
            "base_id": 102,
            "user_id": "user_002",
            "review_text": "äº‘å°å¼€æœºè‡ªæ£€å¤±è´¥ï¼Œç”»é¢ä¸€ç›´æŠ–åŠ¨ï¼Œé‡å¯åé—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œæ€€ç–‘æ˜¯ç¡¬ä»¶è´¨é‡é—®é¢˜ã€‚å·²ç»è¿”ä¿®ä¸€æ¬¡äº†ï¼Œè¿˜æ˜¯åŒæ ·çš„é—®é¢˜ã€‚",
            "rating": 1
        },
        # æ¡ˆä¾‹ 3ï¼šç”¨æˆ·è¯¯è§£ - å¤œé—´é£è¡Œé¿éšœå¤±æ•ˆ
        {
            "base_id": 103,
            "user_id": "user_003",
            "review_text": "å¤œé—´é£è¡Œæ—¶é¿éšœåŠŸèƒ½å®Œå…¨å¤±æ•ˆï¼Œå·®ç‚¹æ’å¢™ï¼Œè¯´æ˜ä¹¦ä¸Šä¹Ÿæ²¡æ˜ç¡®è¯´æ˜å¤œé—´ä¸æ”¯æŒé¿éšœã€‚",
            "rating": 2
        },
        # æ¡ˆä¾‹ 4ï¼šç”¨æˆ·è¯¯è§£ - è¿åŠ¨æ¨¡å¼ä¸‹æ— æ³•é¿éšœ
        {
            "base_id": 104,
            "user_id": "user_004",
            "review_text": "è¿åŠ¨æ¨¡å¼ä¸‹é¿éšœåŠŸèƒ½ä¸å·¥ä½œï¼Œå·®ç‚¹æ’æ ‘ã€‚è¯´æ˜ä¹¦é‡Œæ²¡æœ‰æ˜ç¡®è¯´æ˜è¿åŠ¨æ¨¡å¼ä¼šå…³é—­é¿éšœï¼Œè¿™æ˜¯è®¾è®¡ç¼ºé™·è¿˜æ˜¯æˆ‘ç†è§£é”™äº†ï¼Ÿ",
            "rating": 2
        },
        # æ¡ˆä¾‹ 5ï¼šæ— å…³å™ªéŸ³ - å¿«é€’æ…¢ï¼ˆåº”åœ¨ Filter é˜¶æ®µè¢«è¿‡æ»¤ï¼Œæˆ–å½’ä¸º Otherï¼‰
        {
            "base_id": 105,
            "user_id": "user_005",
            "review_text": "å¿«é€’åŒ…è£…ç ´æŸï¼Œç­‰äº†å¾ˆä¹…æ‰æ”¶åˆ°ï¼Œç‰©æµä½“éªŒå¾ˆå·®ã€‚",
            "rating": 2
        }
    ],
    # æ­£é¢è¯„è®ºæ± ï¼ˆrating 4-5ï¼‰
    "positive": [
        {
            "base_id": 201,
            "user_id": "user_101",
            "review_text": "äº§å“éå¸¸æ»¡æ„ï¼ç”»è´¨æ¸…æ™°ï¼Œç¨³å®šæ€§å¾ˆå¥½ï¼Œç»­èˆªä¹Ÿè¾¾åˆ°äº†å®£ä¼ çš„æ ‡å‡†ã€‚æ“ä½œç®€å•ï¼Œæ–°æ‰‹ä¹Ÿèƒ½å¿«é€Ÿä¸Šæ‰‹ã€‚å¼ºçƒˆæ¨èï¼",
            "rating": 5
        },
        {
            "base_id": 202,
            "user_id": "user_102",
            "review_text": "æ€§ä»·æ¯”å¾ˆé«˜ï¼ŒåŠŸèƒ½é½å…¨ï¼Œé¿éšœç³»ç»Ÿå¾ˆçµæ•ï¼Œæ‹æ‘„æ•ˆæœè¶…å‡ºé¢„æœŸã€‚å®¢æœæ€åº¦ä¹Ÿå¾ˆå¥½ï¼Œæœ‰é—®é¢˜åŠæ—¶è§£å†³ã€‚",
            "rating": 5
        },
        {
            "base_id": 203,
            "user_id": "user_103",
            "review_text": "æ•´ä½“ä½“éªŒä¸é”™ï¼Œç”»è´¨æ¸…æ™°ï¼Œäº‘å°ç¨³å®šï¼Œç”µæ± ç»­èˆªåŸºæœ¬ç¬¦åˆé¢„æœŸã€‚è™½ç„¶æœ‰äº›å°é—®é¢˜ï¼Œä½†æ€»ä½“æ»¡æ„ã€‚",
            "rating": 4
        },
        {
            "base_id": 204,
            "user_id": "user_104",
            "review_text": "äº§å“åšå·¥ç²¾ç»†ï¼Œé£è¡Œç¨³å®šï¼Œæ‹æ‘„æ•ˆæœå¾ˆå¥½ã€‚è¯´æ˜ä¹¦æ¸…æ™°æ˜“æ‡‚ï¼Œä¸Šæ‰‹å¾ˆå¿«ã€‚å€¼å¾—è´­ä¹°ï¼",
            "rating": 4
        }
    ],
    # ä¸­æ€§è¯„è®ºæ± ï¼ˆrating 3ï¼‰
    "neutral": [
        {
            "base_id": 301,
            "user_id": "user_201",
            "review_text": "äº§å“è¿˜å¯ä»¥ï¼Œç”»è´¨ä¸€èˆ¬ï¼Œç¨³å®šæ€§è¿˜è¡Œã€‚ä»·æ ¼é€‚ä¸­ï¼Œä½†åŠŸèƒ½æ²¡æœ‰ç‰¹åˆ«çªå‡ºçš„åœ°æ–¹ã€‚",
            "rating": 3
        }
    ]
}


# ==================== çŠ¶æ€å®šä¹‰ ====================
class ReviewState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    raw_reviews: List[dict]  # æ–°è¯„è®º
    critical_reviews: List[dict]  # ç­›é€‰åçš„é«˜å±è¯„è®º
    rag_analysis_results: List[dict]  # å½’å› ç»“æœ
    action_plans: List[dict]  # è¡ŒåŠ¨å»ºè®®
    logs: List[str]  # æ—¥å¿—ï¼ˆä½¿ç”¨ operator.add è¿½åŠ ï¼‰
    processed_ids: List[str]  # å·²å¤„ç†çš„è¯„è®ºIDé›†åˆï¼ˆç”¨äºå¹‚ç­‰æ€§å»é‡ï¼‰


# ==================== çŠ¶æ€ Reducer ====================
# å®šä¹‰çŠ¶æ€åˆå¹¶è§„åˆ™
def reducer(state: ReviewState, update: ReviewState) -> ReviewState:
    """åˆå¹¶çŠ¶æ€æ›´æ–°"""
    # å¯¹äºåˆ—è¡¨ç±»å‹ï¼Œä½¿ç”¨ operator.add è¿½åŠ 
    # å¯¹äºå…¶ä»–ç±»å‹ï¼Œç›´æ¥è¦†ç›–
    merged = state.copy()
    
    # åˆå¹¶åˆ—è¡¨ï¼ˆè¿½åŠ ï¼‰
    if "logs" in update:
        merged["logs"] = state.get("logs", []) + update.get("logs", [])
    if "raw_reviews" in update:
        merged["raw_reviews"] = update.get("raw_reviews", [])
    if "critical_reviews" in update:
        merged["critical_reviews"] = update.get("critical_reviews", [])
    if "rag_analysis_results" in update:
        merged["rag_analysis_results"] = update.get("rag_analysis_results", [])
    if "action_plans" in update:
        merged["action_plans"] = update.get("action_plans", [])
    if "processed_ids" in update:
        # åˆå¹¶å·²å¤„ç†IDé›†åˆï¼ˆå»é‡ï¼‰
        existing_ids = set(state.get("processed_ids", []))
        new_ids = set(update.get("processed_ids", []))
        merged["processed_ids"] = list(existing_ids | new_ids)
    
    return merged


# ==================== åˆå§‹åŒ– LLM ====================
def init_llm():
    """åˆå§‹åŒ– LLM"""
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise ValueError("DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    
    return ChatTongyi(
        model="qwen-plus",
        temperature=0,
        dashscope_api_key=api_key
    )


# ==================== èŠ‚ç‚¹å®šä¹‰ ====================

def node_monitor(state: ReviewState) -> ReviewState:
    """
    èŠ‚ç‚¹ 1: ç›‘æ§æ–°è¯„è®º
    åŠ¨æ€æ¨¡æ‹Ÿç”Ÿæˆå™¨ï¼šä» MOCK_DATA_POOL éšæœºé‡‡æ ·ï¼Œå¹¶æ·»åŠ å¾®ç§’çº§æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§
    å®ç°å¹‚ç­‰æ€§ï¼šæ£€æŸ¥å·²å¤„ç†çš„IDï¼Œé¿å…é‡å¤å¤„ç†
    
    æµ‹è¯•ä¼˜åŒ–ï¼šç¡®ä¿æ¯æ¬¡å¢é‡ >= 2 æ¡è¯„è®ºï¼Œå…¶ä¸­è‡³å°‘ 1 æ¡ä¸ºæ­£é¢è¯„è®º
    """
    # è·å–å·²å¤„ç†çš„IDé›†åˆï¼ˆç”¨äºå»é‡ï¼‰
    processed_ids = set(state.get("processed_ids", []))
    
    # ä½¿ç”¨å¾®ç§’çº§æ—¶é—´æˆ³ï¼ˆtime.time_ns()ï¼‰ç¡®ä¿æ¯æ¬¡è¿è¡Œç”Ÿæˆçš„IDç»å¯¹å”¯ä¸€
    # è¿™æ ·å¯ä»¥ç»•è¿‡åç»­èŠ‚ç‚¹çš„å»é‡é€»è¾‘ï¼Œä¿è¯æ¼”ç¤ºæ—¶æ¯æ¬¡ç‚¹å‡»å¿…æœ‰æ–°ç»“æœ
    current_timestamp_ns = time.time_ns()  # çº³ç§’çº§æ—¶é—´æˆ³ï¼Œç¡®ä¿å”¯ä¸€æ€§
    new_reviews = []
    new_processed_ids = []
    
    # æµ‹è¯•ä¼˜åŒ–ï¼šç¡®ä¿æ¯æ¬¡è‡³å°‘ç”Ÿæˆ 2 æ¡è¯„è®ºï¼Œä¸”è‡³å°‘åŒ…å« 1 æ¡æ­£é¢è¯„è®º
    # 1. é¦–å…ˆç¡®ä¿è‡³å°‘é€‰æ‹© 1 æ¡æ­£é¢è¯„è®º
    if MOCK_DATA_POOL["positive"]:
        positive_template = random.choice(MOCK_DATA_POOL["positive"])
        unique_suffix = f"{current_timestamp_ns}_{random.randint(1000, 9999)}"
        review_id = f"{positive_template['base_id']}_{unique_suffix}"
        
        if review_id not in processed_ids:
            review = {
                "review_id": review_id,
                "user_id": positive_template['user_id'],
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                "review_text": positive_template['review_text'],
                "rating": positive_template['rating']
            }
            new_reviews.append(review)
            new_processed_ids.append(review_id)
    
    # 2. å†ä»è´Ÿé¢æˆ–ä¸­æ€§è¯„è®ºä¸­éšæœºé€‰æ‹©è‡³å°‘ 1 æ¡ï¼ˆç¡®ä¿æ€»æ•° >= 2ï¼‰
    remaining_needed = max(1, 2 - len(new_reviews))  # è‡³å°‘è¿˜éœ€è¦ 1 æ¡ï¼Œç¡®ä¿æ€»æ•° >= 2
    all_other_templates = MOCK_DATA_POOL["negative"] + MOCK_DATA_POOL["neutral"]
    
    if all_other_templates:
        # éšæœºé€‰æ‹©å‰©ä½™éœ€è¦çš„è¯„è®ºæ•°é‡ï¼ˆå¯ä»¥å¤šé€‰å‡ æ¡å¢åŠ éšæœºæ€§ï¼‰
        additional_count = random.randint(remaining_needed, min(remaining_needed + 1, len(all_other_templates)))
        sampled_others = random.sample(all_other_templates, min(additional_count, len(all_other_templates)))
        
        for template in sampled_others:
            unique_suffix = f"{current_timestamp_ns}_{random.randint(1000, 9999)}"
            review_id = f"{template['base_id']}_{unique_suffix}"
            
            # å¹‚ç­‰æ€§æ£€æŸ¥ï¼šå¦‚æœIDå·²å¤„ç†ï¼Œè·³è¿‡
            if review_id in processed_ids:
                continue
            
            review = {
                "review_id": review_id,
                "user_id": template['user_id'],
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                "review_text": template['review_text'],
                "rating": template['rating']
            }
            new_reviews.append(review)
            new_processed_ids.append(review_id)
    
    # æ¨¡æ‹Ÿæ—¶é—´æ¨è¿›æ„Ÿ
    current_time_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    positive_count = sum(1 for r in new_reviews if r.get('rating', 0) >= 4)
    negative_count = sum(1 for r in new_reviews if r.get('rating', 0) < 3)
    neutral_count = len(new_reviews) - positive_count - negative_count
    log_message = f"ğŸ“… æ¨¡æ‹Ÿæ—¶é—´æ¨è¿›ï¼š{current_time_str} | æ£€æµ‹åˆ° {len(new_reviews)} æ¡æ–°å¢è¯„è®º"
    log_message += f" (æ­£é¢: {positive_count} æ¡, è´Ÿé¢: {negative_count} æ¡, ä¸­æ€§: {neutral_count} æ¡)"
    if new_reviews:
        log_message += f" | ID: {[r['review_id'] for r in new_reviews]}"
    
    return {
        "raw_reviews": new_reviews,
        "processed_ids": new_processed_ids,
        "logs": [log_message]
    }


def node_filter(state: ReviewState) -> ReviewState:
    """
    èŠ‚ç‚¹ 2: ç­›é€‰é«˜å±è¯„è®º
    ä½¿ç”¨ LLM åˆ¤æ–­æ˜¯å¦åŒ…å«"æ•…éšœ/å®‰å…¨/è´¨é‡"å…³é”®è¯
    """
    llm = init_llm()
    raw_reviews = state.get("raw_reviews", [])
    
    if not raw_reviews:
        log_message = "âš ï¸ ç­›é€‰èŠ‚ç‚¹ï¼šæ— æ–°è¯„è®ºéœ€è¦ç­›é€‰"
        return {
            "critical_reviews": [],
            "logs": [log_message]
        }
    
    # æ„å»ºç­›é€‰ promptï¼ŒåŒ…å«å®Œæ•´çš„ review_id
    reviews_text = "\n".join([
        f"è¯„è®ºID {review['review_id']}: {review['review_text']} (è¯„åˆ†: {review['rating']})"
        for i, review in enumerate(raw_reviews)
    ])
    
    # æå–æ‰€æœ‰ review_id ä¾›å‚è€ƒ
    all_review_ids = [review['review_id'] for review in raw_reviews]
    
    filter_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¯„è®ºï¼Œç­›é€‰å‡ºåŒ…å«"æ•…éšœ/å®‰å…¨/è´¨é‡é—®é¢˜"çš„é«˜å±è¯„è®ºã€‚

è¯„è®ºåˆ—è¡¨ï¼š
{reviews_text}

ç­›é€‰æ ‡å‡†ï¼ˆæ»¡è¶³ä»»ä¸€æ¡ä»¶å³è§†ä¸ºé«˜å±ï¼‰ï¼š
1. è¯„åˆ†ä½äº3æ˜Ÿï¼ˆrating < 3ï¼‰
2. åŒ…å«æ•…éšœã€å¤±æ•ˆã€å®‰å…¨é—®é¢˜ã€è´¨é‡é—®é¢˜ç­‰å…³é”®è¯
3. æ¶‰åŠäº§å“ç¼ºé™·æˆ–å®‰å…¨éšæ‚£ï¼ˆå¦‚ï¼šé¿éšœå¤±æ•ˆã€äº‘å°æŠ–åŠ¨ã€åŠŸèƒ½ä¸å·¥ä½œç­‰ï¼‰

è¯·è¿”å› JSON æ ¼å¼ï¼ŒåŒ…å«ï¼š
{{
  "critical_review_ids": [è¯„è®ºIDåˆ—è¡¨ï¼Œå¿…é¡»ä½¿ç”¨å®Œæ•´çš„review_idï¼Œä¾‹å¦‚: {all_review_ids[:2] if len(all_review_ids) >= 2 else all_review_ids}],
  "reason": "ç­›é€‰åŸå› "
}}

é‡è¦ï¼š
- å¿…é¡»ä½¿ç”¨å®Œæ•´çš„ review_idï¼ˆåŒ…å«æ—¶é—´æˆ³éƒ¨åˆ†ï¼‰
- è¯·ç¡®ä¿åŒ…å«æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é«˜å±è¯„è®ºID
- åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜"""
    
    try:
        response = llm.invoke([HumanMessage(content=filter_prompt)])
        answer = response.content if hasattr(response, 'content') else str(response)
        
        # è§£æ JSON
        json_str = answer.strip()
        if json_str.startswith("```json"):
            json_str = json_str[7:]
        elif json_str.startswith("```"):
            json_str = json_str[3:]
        if json_str.endswith("```"):
            json_str = json_str[:-3]
        json_str = json_str.strip()
        
        result = json.loads(json_str)
        critical_ids = result.get("critical_review_ids", [])
        
        # ç­›é€‰å‡ºé«˜å±è¯„è®ºï¼ˆæ”¯æŒå®Œæ•´IDæˆ–base_idåŒ¹é…ï¼‰
        critical_reviews = []
        for review in raw_reviews:
            review_id = review.get("review_id", "")
            # å°è¯•å®Œæ•´IDåŒ¹é…
            if review_id in critical_ids:
                critical_reviews.append(review)
            else:
                # å°è¯•base_idåŒ¹é…ï¼ˆå¦‚æœLLMè¿”å›çš„æ˜¯æ•°å­—IDï¼‰
                base_id = review_id.split("_")[0] if "_" in review_id else review_id
                if str(base_id) in [str(cid) for cid in critical_ids] or base_id in [str(cid) for cid in critical_ids]:
                    critical_reviews.append(review)
        
        log_message = f"ğŸ” ç­›é€‰èŠ‚ç‚¹ï¼šä» {len(raw_reviews)} æ¡è¯„è®ºä¸­ç­›é€‰å‡º {len(critical_reviews)} æ¡é«˜å±è¯„è®º"
        if critical_reviews:
            log_message += f" (ID: {[r.get('review_id') for r in critical_reviews]})"
        elif critical_ids:
            log_message += f" | LLMè¿”å›çš„ID: {critical_ids}ï¼Œä½†åŒ¹é…å¤±è´¥"
        
        return {
            "critical_reviews": critical_reviews,
            "logs": [log_message]
        }
        
    except Exception as e:
        # å¦‚æœ LLM ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨é™çº§è§„åˆ™ï¼šrating < 3 æˆ–åŒ…å«å…³é”®è¯
        keywords = ["æ•…éšœ", "å¤±æ•ˆ", "é—®é¢˜", "å", "ä¸å·¥ä½œ", "å®‰å…¨", "å±é™©", "è´¨é‡", "é¿éšœ", "æŠ–åŠ¨", "ä¸ç¨³å®š", "æ’", "å·®ç‚¹", "è™šæ ‡", "æ¬ºéª—"]
        critical_reviews = []
        
        for review in raw_reviews:
            rating = review.get("rating", 5)
            review_text = review.get("review_text", "")
            
            # è¯„åˆ†ä½äº3æ˜Ÿï¼Œæˆ–è€…åŒ…å«å…³é”®è¯
            if rating < 3 or any(keyword in review_text for keyword in keywords):
                critical_reviews.append(review)
        
        log_message = f"ğŸ” ç­›é€‰èŠ‚ç‚¹ï¼ˆé™çº§æ¨¡å¼ï¼‰ï¼šç­›é€‰å‡º {len(critical_reviews)} æ¡é«˜å±è¯„è®º"
        if critical_reviews:
            log_message += f" (ID: {[r.get('review_id') for r in critical_reviews]})"
        log_message += f" | LLMé”™è¯¯: {str(e)[:50]}"
        
        return {
            "critical_reviews": critical_reviews,
            "logs": [log_message]
        }


def node_rag_analysis(state: ReviewState) -> ReviewState:
    """
    èŠ‚ç‚¹ 3: RAG å½’å› åˆ†æ
    æ¥å…¥çœŸå®çš„å‘é‡æ£€ç´¢ï¼ŒåŸºäºäº§å“è¯´æ˜ä¹¦è¿›è¡Œå½’å› åˆ†æ
    """
    llm = init_llm()
    critical_reviews = state.get("critical_reviews", [])
    
    if not critical_reviews:
        log_message = "âš ï¸ RAG åˆ†æèŠ‚ç‚¹ï¼šæ— é«˜å±è¯„è®ºéœ€è¦åˆ†æ"
        return {
            "rag_analysis_results": [],
            "logs": [log_message]
        }
    
    # åˆå§‹åŒ–å‘é‡åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
    vectorstore = None
    try:
        from langchain_community.vectorstores import Chroma
        from langchain_community.embeddings import DashScopeEmbeddings
        import os
        
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if api_key:
            embeddings = DashScopeEmbeddings(
                model="text-embedding-v3",
                dashscope_api_key=api_key
            )
            vectorstore = Chroma(
                persist_directory="./chroma_db",
                embedding_function=embeddings
            )
    except Exception as e:
        log_message = f"âš ï¸ å‘é‡åº“åˆå§‹åŒ–å¤±è´¥: {str(e)[:50]}"
        # ç»§ç»­æ‰§è¡Œï¼Œä½¿ç”¨é™çº§é€»è¾‘
    
    rag_results = []
    
    for review in critical_reviews:
        review_text = review.get("review_text", "")
        review_id = review.get("review_id", "")
        
        try:
            # å¦‚æœæœ‰å‘é‡åº“ï¼Œä½¿ç”¨çœŸå®çš„ RAG æ£€ç´¢
            if vectorstore:
                # æ„å»ºæŸ¥è¯¢
                query = f"ç”¨æˆ·åé¦ˆï¼š{review_text}ã€‚è¯·åˆ†æè¿™æ˜¯äº§å“å·²çŸ¥å±€é™è¿˜æ˜¯æ–°é—®é¢˜ã€‚"
                
                # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                try:
                    docs_with_scores = vectorstore.similarity_search_with_score(query, k=5)
                    # è¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ
                    relevant_docs = []
                    for doc, distance in docs_with_scores:
                        if distance < 1.5:  # è·ç¦»é˜ˆå€¼
                            relevant_docs.append(doc)
                    
                    if relevant_docs:
                        # æ„å»ºä¸Šä¸‹æ–‡
                        context = "\n\n".join([doc.page_content[:300] for doc in relevant_docs[:3]])
                        
                        # ä½¿ç”¨ RAG å¢å¼ºçš„ Prompt
                        rag_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“åˆ†æå¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·åé¦ˆå’Œäº§å“è¯´æ˜ä¹¦ï¼Œè¿›è¡Œå‡†ç¡®çš„å½’å› åˆ†æã€‚

äº§å“è¯´æ˜ä¹¦ç›¸å…³å†…å®¹ï¼š
{context}

ç”¨æˆ·åé¦ˆï¼š{review_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "review_id": "{review_id}",
  "conclusion": "âœ… äº§å“å·²çŸ¥å±€é™" æˆ– "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥" æˆ– "â“ ç”¨æˆ·ä½¿ç”¨é—®é¢˜",
  "reason": "åŸºäºäº§å“è¯´æ˜ä¹¦çš„åˆ†æåŸå› ",
  "evidence": "ä»è¯´æ˜ä¹¦ä¸­æå–çš„ç›¸å…³è¯æ®ç‰‡æ®µ"
}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚"""
                    else:
                        # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                        rag_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·åé¦ˆï¼Œåˆ¤æ–­è¿™æ˜¯ç”¨æˆ·ä½¿ç”¨é—®é¢˜è¿˜æ˜¯äº§å“ç¼ºé™·ã€‚

ç”¨æˆ·åé¦ˆï¼š{review_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "review_id": "{review_id}",
  "conclusion": "âœ… äº§å“å·²çŸ¥å±€é™" æˆ– "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥" æˆ– "â“ ç”¨æˆ·ä½¿ç”¨é—®é¢˜",
  "reason": "åˆ†æåŸå› ",
  "evidence": "æœªåœ¨è¯´æ˜ä¹¦ä¸­æ‰¾åˆ°ç›¸å…³æè¿°"
}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚"""
                except Exception as e:
                    # å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                    rag_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·åé¦ˆï¼Œåˆ¤æ–­è¿™æ˜¯ç”¨æˆ·ä½¿ç”¨é—®é¢˜è¿˜æ˜¯äº§å“ç¼ºé™·ã€‚

ç”¨æˆ·åé¦ˆï¼š{review_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "review_id": "{review_id}",
  "conclusion": "âœ… äº§å“å·²çŸ¥å±€é™" æˆ– "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥" æˆ– "â“ ç”¨æˆ·ä½¿ç”¨é—®é¢˜",
  "reason": "åˆ†æåŸå› ",
  "evidence": "å‘é‡æ£€ç´¢å¤±è´¥: {str(e)[:50]}"
}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚"""
            else:
                # æ²¡æœ‰å‘é‡åº“ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                rag_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·åé¦ˆï¼Œåˆ¤æ–­è¿™æ˜¯ç”¨æˆ·ä½¿ç”¨é—®é¢˜è¿˜æ˜¯äº§å“ç¼ºé™·ã€‚

ç”¨æˆ·åé¦ˆï¼š{review_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "review_id": "{review_id}",
  "conclusion": "âœ… äº§å“å·²çŸ¥å±€é™" æˆ– "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥" æˆ– "â“ ç”¨æˆ·ä½¿ç”¨é—®é¢˜",
  "reason": "åˆ†æåŸå› ",
  "evidence": "å‘é‡åº“æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ"
}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚"""
            
            # è°ƒç”¨ LLM
            response = llm.invoke([HumanMessage(content=rag_prompt)])
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æ JSONï¼ˆæ”¹è¿›çš„è§£æé€»è¾‘ï¼‰
            json_str = answer.strip()
            
            # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
            if json_str.startswith("```json"):
                json_str = json_str[7:]
            elif json_str.startswith("```"):
                json_str = json_str[3:]
            if json_str.endswith("```"):
                json_str = json_str[:-3]
            json_str = json_str.strip()
            
            # å°è¯•æå– JSONï¼ˆå¤„ç†å¯èƒ½çš„é¢å¤–æ–‡æœ¬ï¼‰
            if "{" in json_str and "}" in json_str:
                start_idx = json_str.find("{")
                end_idx = json_str.rfind("}") + 1
                json_str = json_str[start_idx:end_idx]
            
            result = json.loads(json_str)
            
            rag_results.append({
                "review_id": review_id,
                "review_text": review_text,
                "conclusion": result.get("conclusion", "â“ éœ€è¦äººå·¥åˆ¤æ–­"),
                "reason": result.get("reason", ""),
                "evidence": result.get("evidence", "")
            })
            
        except json.JSONDecodeError as e:
            # JSON è§£æå¤±è´¥ï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
            rag_results.append({
                "review_id": review_id,
                "review_text": review_text,
                "conclusion": "â“ éœ€è¦äººå·¥åˆ¤æ–­",
                "reason": f"JSON è§£æå¤±è´¥: {str(e)[:100]}",
                "evidence": f"LLM è¿”å›å†…å®¹: {answer[:200]}"
            })
        except Exception as e:
            # å…¶ä»–é”™è¯¯
            rag_results.append({
                "review_id": review_id,
                "review_text": review_text,
                "conclusion": "â“ éœ€è¦äººå·¥åˆ¤æ–­",
                "reason": f"RAG åˆ†æå¤±è´¥: {str(e)[:100]}",
                "evidence": ""
            })
    
    log_message = f"ğŸ“„ RAG åˆ†æèŠ‚ç‚¹ï¼šå®Œæˆ {len(rag_results)} æ¡è¯„è®ºçš„å½’å› åˆ†æ"
    if vectorstore:
        log_message += "ï¼ˆå·²ä½¿ç”¨å‘é‡æ£€ç´¢ï¼‰"
    else:
        log_message += "ï¼ˆä½¿ç”¨åŸºç¡€åˆ†æï¼‰"
    
    return {
        "rag_analysis_results": rag_results,
        "logs": [log_message]
    }


def node_action_gen(state: ReviewState) -> ReviewState:
    """
    èŠ‚ç‚¹ 4: ç”Ÿæˆè¡ŒåŠ¨å»ºè®®
    åŸºäºå½’å› ç”Ÿæˆ JSON æ ¼å¼çš„ Action
    """
    llm = init_llm()
    rag_results = state.get("rag_analysis_results", [])
    
    if not rag_results:
        log_message = "âš ï¸ è¡ŒåŠ¨ç”ŸæˆèŠ‚ç‚¹ï¼šæ— å½’å› ç»“æœéœ€è¦ç”Ÿæˆè¡ŒåŠ¨"
        return {
            "action_plans": [],
            "logs": [log_message]
        }
    
    action_plans = []
    
    for rag_result in rag_results:
        review_text = rag_result.get("review_text", "")
        conclusion = rag_result.get("conclusion", "")
        reason = rag_result.get("reason", "")
        evidence = rag_result.get("evidence", "")
        
        # ç”Ÿæˆè¡ŒåŠ¨å»ºè®®ï¼ˆåŸºäº RAG å½’å› ç»“æœï¼‰
        action_prompt = f"""åŸºäºä»¥ä¸‹å½’å› åˆ†æï¼Œç”Ÿæˆå…·ä½“çš„è¡ŒåŠ¨å»ºè®®ã€‚

ç”¨æˆ·åé¦ˆï¼š{review_text}
å½’å› ç»“è®ºï¼š{conclusion}
åˆ†æåŸå› ï¼š{reason}
ç›¸å…³è¯æ®ï¼š{evidence if evidence else "æ— "}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "action_type": "Jira Ticket" æˆ– "Doc Update" æˆ– "Email Draft" æˆ– "Meeting",
  "title": "è¡ŒåŠ¨æ ‡é¢˜",
  "content": "è¯¦ç»†å†…å®¹ï¼ˆåŒ…å«ç”¨æˆ·åé¦ˆã€å½’å› ç»“è®ºå’Œå»ºè®®æªæ–½ï¼‰",
  "priority": "High" æˆ– "Medium" æˆ– "Low"
}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚"""
        
        try:
            response = llm.invoke([HumanMessage(content=action_prompt)])
            answer = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æ JSONï¼ˆæ”¹è¿›çš„è§£æé€»è¾‘ï¼‰
            json_str = answer.strip()
            
            # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
            if json_str.startswith("```json"):
                json_str = json_str[7:]
            elif json_str.startswith("```"):
                json_str = json_str[3:]
            if json_str.endswith("```"):
                json_str = json_str[:-3]
            json_str = json_str.strip()
            
            # å°è¯•æå– JSONï¼ˆå¤„ç†å¯èƒ½çš„é¢å¤–æ–‡æœ¬ï¼‰
            if "{" in json_str and "}" in json_str:
                start_idx = json_str.find("{")
                end_idx = json_str.rfind("}") + 1
                json_str = json_str[start_idx:end_idx]
            
            result = json.loads(json_str)
            action_plans.append({
                "review_id": rag_result.get("review_id"),
                "action_type": result.get("action_type", "Jira Ticket"),
                "title": result.get("title", ""),
                "content": result.get("content", ""),
                "priority": result.get("priority", "Medium")
            })
            
        except Exception as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            action_plans.append({
                "review_id": rag_result.get("review_id"),
                "action_type": "Jira Ticket",
                "title": f"å¤„ç†è¯„è®º {rag_result.get('review_id')} çš„é—®é¢˜",
                "content": review_text,
                "priority": "Medium"
            })
    
    log_message = f"ğŸ’¡ è¡ŒåŠ¨ç”ŸæˆèŠ‚ç‚¹ï¼šç”Ÿæˆ {len(action_plans)} ä¸ªè¡ŒåŠ¨å»ºè®®"
    
    return {
        "action_plans": action_plans,
        "logs": [log_message]
    }


# ==================== æ¡ä»¶è·¯ç”± ====================
def should_continue_analysis(state: ReviewState) -> Literal["rag_analysis", "end"]:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­ RAG åˆ†æ"""
    critical_reviews = state.get("critical_reviews", [])
    if len(critical_reviews) > 0:
        return "rag_analysis"
    return "end"


# ==================== æ„å»ºå›¾ ====================
def build_graph():
    """æ„å»º LangGraph å·¥ä½œæµ"""
    # åˆ›å»ºçŠ¶æ€å›¾ï¼ŒæŒ‡å®š reducer
    workflow = StateGraph(ReviewState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("monitor", node_monitor)
    workflow.add_node("filter", node_filter)
    workflow.add_node("rag_analysis", node_rag_analysis)
    workflow.add_node("action_gen", node_action_gen)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("monitor")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("monitor", "filter")
    
    # æ¡ä»¶è·¯ç”±ï¼šfilter ååˆ¤æ–­æ˜¯å¦ç»§ç»­
    workflow.add_conditional_edges(
        "filter",
        should_continue_analysis,
        {
            "rag_analysis": "rag_analysis",
            "end": END
        }
    )
    
    workflow.add_edge("rag_analysis", "action_gen")
    workflow.add_edge("action_gen", END)
    
    # ç¼–è¯‘å›¾
    graph_app = workflow.compile()
    
    return graph_app


# ==================== å¯¼å‡º ====================
# åˆ›å»ºå…¨å±€å›¾å®ä¾‹
graph_app = build_graph()


if __name__ == "__main__":
    # æµ‹è¯•å·¥ä½œæµ
    initial_state = {
        "raw_reviews": [],
        "critical_reviews": [],
        "rag_analysis_results": [],
        "action_plans": [],
        "logs": []
    }
    
    print("ğŸš€ å¼€å§‹è¿è¡Œ ReviewOps å·¥ä½œæµ...")
    result = graph_app.invoke(initial_state)
    
    print("\nğŸ“Š æœ€ç»ˆçŠ¶æ€ï¼š")
    print(f"åŸå§‹è¯„è®ºæ•°: {len(result.get('raw_reviews', []))}")
    print(f"é«˜å±è¯„è®ºæ•°: {len(result.get('critical_reviews', []))}")
    print(f"å½’å› ç»“æœæ•°: {len(result.get('rag_analysis_results', []))}")
    print(f"è¡ŒåŠ¨å»ºè®®æ•°: {len(result.get('action_plans', []))}")
    
    print("\nğŸ“ æ—¥å¿—ï¼š")
    for log in result.get("logs", []):
        print(f"  {log}")
    
    print("\nğŸ’¡ è¡ŒåŠ¨å»ºè®®ï¼š")
    for action in result.get("action_plans", []):
        print(f"  - {action.get('title')} ({action.get('action_type')})")

