"""
ReviewOps - ç”¨æˆ·åé¦ˆå†³ç­–ä¸­å°
ä¸€ä¸ªå¸®åŠ©äº§å“ç»ç†åˆ†æç”¨æˆ·åé¦ˆçš„ Bç«¯ SaaS åŸå‹
"""

import streamlit as st
import pandas as pd
import time
from collections import Counter
import re
import plotly.express as px
import plotly.graph_objects as go
import os
import json
from enum import Enum
from typing import Optional
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# RAG ç›¸å…³å¯¼å…¥
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.chat_models import ChatTongyi
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

# Pydantic æ¨¡å‹ï¼ˆå¦‚æœæœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€å­—å…¸ï¼‰
try:
    from pydantic import BaseModel
    PYDANTIC_AVAILABLE = True
except ImportError:
    PYDANTIC_AVAILABLE = False
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ BaseModel æ›¿ä»£
    class BaseModel:
        pass

# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="ReviewOps Â· ç”¨æˆ·åé¦ˆå†³ç­–ä¸­å°",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== è‡ªå®šä¹‰æ ·å¼ ====================
st.markdown("""
<style>
    /* ä¸»é¢˜è‰²å½©ç³»ç»Ÿ */
    :root {
        --primary: #6366f1;
        --secondary: #8b5cf6;
        --accent: #06b6d4;
        --success: #10b981;
        --warning: #f59e0b;
        --danger: #ef4444;
    }
    
    /* æŒ‡æ ‡å¡ç‰‡æ ·å¼ */
    [data-testid="stMetric"] {
        background: linear-gradient(135deg, #1e1b4b 0%, #312e81 100%);
        padding: 1.2rem;
        border-radius: 12px;
        border: 1px solid rgba(99, 102, 241, 0.3);
        box-shadow: 0 4px 20px rgba(99, 102, 241, 0.15);
    }
    
    [data-testid="stMetric"] label {
        color: #a5b4fc !important;
        font-weight: 500;
    }
    
    [data-testid="stMetric"] [data-testid="stMetricValue"] {
        color: #e0e7ff !important;
        font-weight: 700;
    }
    
    /* ä¾§è¾¹æ æ ·å¼ */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #0f0a1f 0%, #1a1333 100%);
    }
    
    [data-testid="stSidebar"] .stMarkdown h1,
    [data-testid="stSidebar"] .stMarkdown h2,
    [data-testid="stSidebar"] .stMarkdown h3 {
        color: #c4b5fd;
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {
        background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 8px;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(99, 102, 241, 0.4);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 25px rgba(99, 102, 241, 0.5);
    }
    
    /* è¡¨æ ¼æ ·å¼ */
    .stDataFrame {
        border-radius: 12px;
        overflow: hidden;
    }
    
    /* Expander æ ·å¼ */
    .streamlit-expanderHeader {
        background: rgba(99, 102, 241, 0.1);
        border-radius: 8px;
    }
    
    /* ä¸»æ ‡é¢˜ */
    .main-title {
        background: linear-gradient(90deg, #6366f1, #8b5cf6, #06b6d4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-size: 2.5rem;
        font-weight: 800;
        margin-bottom: 0;
    }
    
    /* ä¿¡æ¯å¡ç‰‡ */
    .info-card {
        background: linear-gradient(135deg, #1e1b4b 0%, #312e81 100%);
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid rgba(99, 102, 241, 0.3);
        margin: 1rem 0;
    }
    
    /* è¡ŒåŠ¨é¡¹å¡ç‰‡å®¹å™¨ */
    .action-card {
        background: #ffffff;
        border-radius: 12px;
        padding: 1.25rem 1.5rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        border: 1px solid #e5e7eb;
        transition: all 0.2s ease;
    }
    
    .action-card:hover {
        box-shadow: 0 4px 16px rgba(0, 0, 0, 0.12);
        transform: translateY(-2px);
    }
    
    /* é«˜ä¼˜å…ˆçº§ */
    .action-card.high-priority {
        border-left: 4px solid #ef4444;
    }
    
    /* ä¸­ä¼˜å…ˆçº§ */
    .action-card.medium-priority {
        border-left: 4px solid #f59e0b;
    }
    
    /* å¸¸è§„ä¼˜å…ˆçº§ */
    .action-card.low-priority {
        border-left: 4px solid #10b981;
    }
    
    /* ä¼˜å…ˆçº§æ ‡ç­¾ */
    .priority-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 0.75rem;
        font-weight: 600;
        margin-right: 0.5rem;
    }
    
    .priority-badge.high {
        background: #fef2f2;
        color: #dc2626;
    }
    
    .priority-badge.medium {
        background: #fffbeb;
        color: #d97706;
    }
    
    .priority-badge.low {
        background: #ecfdf5;
        color: #059669;
    }
    
    /* è¡ŒåŠ¨æ ‡é¢˜ */
    .action-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #1f2937;
        margin: 0.5rem 0;
    }
    
    /* è¡ŒåŠ¨è¯¦æƒ… */
    .action-detail {
        color: #4b5563;
        font-size: 0.95rem;
        line-height: 1.6;
        margin: 0.75rem 0;
    }
    
    /* å…ƒä¿¡æ¯ */
    .action-meta {
        display: flex;
        gap: 1.5rem;
        margin-top: 1rem;
        padding-top: 0.75rem;
        border-top: 1px solid #f3f4f6;
    }
    
    .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
        color: #6b7280;
        font-size: 0.85rem;
    }
    
    .meta-item strong {
        color: #374151;
    }
    
    /* åˆ†å‰²çº¿ */
    hr {
        border: none;
        height: 1px;
        background: linear-gradient(90deg, transparent, rgba(99, 102, 241, 0.5), transparent);
        margin: 2rem 0;
    }
</style>
""", unsafe_allow_html=True)


# ==================== æ•°æ®åŠ è½½ ====================
@st.cache_data(ttl=60)  # 60ç§’ç¼“å­˜ï¼Œä¾¿äºå¼€å‘
def load_reviews():
    """åŠ è½½ç”¨æˆ·è¯„è®ºæ•°æ®"""
    df = pd.read_csv("user_reviews.csv")
    # ç¡®ä¿æœ‰ review_id åˆ—ï¼ˆå¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨ user_id æˆ–åˆ›å»ºï¼‰
    if 'review_id' not in df.columns:
        if 'user_id' in df.columns:
            df['review_id'] = df['user_id']
        else:
            df['review_id'] = range(1, len(df) + 1)
    return df


# æ¸…é™¤ç¼“å­˜ä»¥ä¾¿é‡æ–°åŠ è½½æ•°æ®
load_reviews.clear()

# åŠ è½½æ•°æ®
reviews_df = load_reviews()


# ==================== RAG åˆå§‹åŒ– ====================
@st.cache_resource
def init_vectorstore(api_key):
    """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
    if not api_key:
        return None
    
    try:
        embeddings = DashScopeEmbeddings(
            model="text-embedding-v3",  # ä¸ injest.py ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨ v3 æ¨¡å‹ï¼ˆ1536 ç»´ï¼‰
            dashscope_api_key=api_key
        )
        
        vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embeddings
        )
        return vectorstore
    except Exception as e:
        st.error(f"å‘é‡åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


@st.cache_resource
def init_llm(api_key):
    """åˆå§‹åŒ– LLM"""
    if not api_key:
        return None
    
    try:
        from langchain_community.chat_models import ChatTongyi
        llm = ChatTongyi(
            model="qwen-plus",
            temperature=0,
            dashscope_api_key=api_key
        )
        return llm
    except Exception as e:
        st.error(f"LLM åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


def perform_rag_query(vectorstore, llm, question):
    """æ‰§è¡Œ RAG æŸ¥è¯¢ï¼šæ£€ç´¢ + ç”Ÿæˆ"""
    if not vectorstore or not llm:
        return None, []
    
    try:
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ similarity_search_with_score è·å–è·ç¦»åˆ†æ•°ï¼‰
        # ä½¿ç”¨æ›´å¤§çš„ k å€¼ï¼Œç„¶åå»é‡å’Œè¿‡æ»¤
        try:
            docs_with_scores = vectorstore.similarity_search_with_score(question, k=10)
        except:
            # å¦‚æœä¸æ”¯æŒ similarity_search_with_scoreï¼Œå›é€€åˆ°æ™®é€šæœç´¢
            docs = vectorstore.similarity_search(question, k=5)
            # ç®€å•å»é‡ï¼Œè¿”å›æ‰€æœ‰ä¸é‡å¤çš„æ–‡æ¡£
            unique_docs = []
            seen_contents = set()
            for doc in docs:
                content_fingerprint = doc.page_content[:150].strip()
                if content_fingerprint not in seen_contents:
                    seen_contents.add(content_fingerprint)
                    unique_docs.append(doc)
            docs = unique_docs  # è¿”å›æ‰€æœ‰å»é‡åçš„æ–‡æ¡£ï¼Œä¸é™åˆ¶æ•°é‡
        else:
            # 2. å»é‡ï¼šåŸºäºæ–‡æ¡£å†…å®¹çš„ç›¸ä¼¼åº¦å»é‡
            # ChromaDB è¿”å›çš„æ˜¯è·ç¦»ï¼ˆdistanceï¼‰ï¼Œè¶Šå°è¶Šç›¸ä¼¼
            # é€šå¸¸è·ç¦» < 1.5 è¡¨ç¤ºæ¯”è¾ƒç›¸å…³
            unique_docs = []
            seen_contents = set()
            max_distance = 1.5  # æœ€å¤§è·ç¦»é˜ˆå€¼ï¼ˆæ ¹æ®å®é™…è°ƒæ•´ï¼‰
            
            for doc, distance in docs_with_scores:
                # è¿‡æ»¤è·ç¦»è¿‡å¤§çš„ç»“æœï¼ˆç›¸ä¼¼åº¦å¤ªä½ï¼‰
                if distance > max_distance:
                    continue
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤ï¼ˆä½¿ç”¨å‰150ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼Œæ›´å‡†ç¡®ï¼‰
                content_fingerprint = doc.page_content[:150].strip()
                if content_fingerprint not in seen_contents:
                    seen_contents.add(content_fingerprint)
                    unique_docs.append(doc)
                    # ä¸å†é™åˆ¶æ•°é‡ï¼Œè¿”å›æ‰€æœ‰ç›¸å…³ä¸”ä¸é‡å¤çš„æ–‡æ¡£
            
            docs = unique_docs  # è¿”å›æ‰€æœ‰ç›¸å…³ä¸”å»é‡åçš„æ–‡æ¡£
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # 3. æ„å»º Prompt
        system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“åˆ†æå¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·åé¦ˆå’Œäº§å“è¯´æ˜ä¹¦ï¼Œè¿›è¡Œå‡†ç¡®çš„å½’å› åˆ†æã€‚

è¯·åŸºäºä»¥ä¸‹äº§å“è¯´æ˜ä¹¦å†…å®¹ï¼Œåˆ†æç”¨æˆ·åé¦ˆé—®é¢˜ï¼š
{context}

å›ç­”æ ¼å¼ï¼š
- è¯´æ˜ä¹¦å¯¹åº”å‚æ•°ï¼š[ä»äº§å“è¯´æ˜ä¹¦ä¸­æå–çš„ç›¸å…³å†…å®¹]
- AI åˆ¤å®šç»“è®ºï¼š[ä½ çš„åˆ¤æ–­ï¼Œå¦‚æœæ˜¯å·²çŸ¥å±€é™ç”¨âœ…ï¼Œå¦‚æœæ˜¯æ–°é—®é¢˜ç”¨âš ï¸ï¼Œå¦‚æœæ˜¯ç”¨æˆ·è¯¯ç”¨ç”¨â“]

å›ç­”ï¼š"""
        
        human_template = "ç”¨æˆ·åé¦ˆï¼š{question}"
        
        messages = [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template(human_template)
        ]
        
        prompt = ChatPromptTemplate.from_messages(messages)
        
        # 4. è°ƒç”¨ LLM
        formatted_prompt = prompt.format_messages(context=context, question=question)
        response = llm.invoke(formatted_prompt)
        
        # 5. æå–å›ç­”
        if hasattr(response, 'content'):
            answer = response.content
        else:
            answer = str(response)
        
        return answer, docs
        
    except Exception as e:
        st.warning(f"RAG æŸ¥è¯¢å‡ºé”™: {e}")
        return None, []


# ==================== è¾…åŠ©å‡½æ•° ====================
def extract_product_name():
    """ä» PDF æ–‡ä»¶åæˆ–å‘é‡åº“ä¸­æå–äº§å“åç§°"""
    # ä» PDF æ–‡ä»¶åæå–ï¼ˆdji_spec.pdf -> DJIï¼‰
    pdf_name = "dji_spec.pdf"
    if os.path.exists(pdf_name):
        # ä»æ–‡ä»¶åæå–äº§å“å
        name = pdf_name.replace("_spec.pdf", "").replace(".pdf", "").upper()
        return name
    return "äº§å“è¯´æ˜ä¹¦"


def calculate_metrics(df):
    """è®¡ç®—å…³é”®æŒ‡æ ‡"""
    total_reviews = len(df)
    avg_rating = df['rating'].mean()
    negative_ratio = len(df[df['rating'] < 3]) / total_reviews * 100
    return total_reviews, avg_rating, negative_ratio


def get_negative_reviews(df):
    """è·å–è´Ÿé¢è¯„ä»·"""
    return df[df['rating'] < 3]


def analyze_reviews_with_llm(reviews_df, llm):
    """
    ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰èšç±»ï¼Œè‡ªåŠ¨å‘ç°ä¸»è¦æŠ±æ€¨ç‚¹
    
    Args:
        reviews_df: åŒ…å« review_text å’Œ user_id (æˆ– review_id) çš„ DataFrame
        llm: LangChain LLM å®ä¾‹
    
    Returns:
        list: åŒ…å« topics çš„åˆ—è¡¨ï¼Œæ¯ä¸ª topic åŒ…å« topic, review_ids, summary
    """
    if reviews_df.empty:
        return []
    
    # ç¡®ä¿æœ‰ review_id æˆ– user_id åˆ—
    if 'review_id' not in reviews_df.columns and 'user_id' in reviews_df.columns:
        reviews_df = reviews_df.copy()
        reviews_df['review_id'] = reviews_df['user_id']
    elif 'review_id' not in reviews_df.columns:
        reviews_df = reviews_df.copy()
        reviews_df['review_id'] = range(1, len(reviews_df) + 1)
    
    # æ„å»ºè¯„è®ºæ–‡æœ¬ï¼ŒåŒ…å« ID ä¿¡æ¯
    review_texts = []
    for idx, row in reviews_df.iterrows():
        review_id = row['review_id']
        review_text = row['review_text'] if 'review_text' in row else row.get('content', '')
        if review_text and not pd.isna(review_text):
            review_texts.append(f"è¯„è®ºID {review_id}: {review_text}")
    
    if not review_texts:
        return []
    
    # æ‹¼æ¥æ‰€æœ‰è¯„è®º
    all_reviews = "\n\n".join(review_texts)
    
    # æ„å»º Prompt
    prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“åé¦ˆåˆ†æå¸ˆã€‚è¯·é˜…è¯»ä»¥ä¸‹ç”¨æˆ·è¯„è®ºï¼Œè‡ªåŠ¨å½’çº³å‡ºå‰ 5 ä¸ªæœ€ä¸¥é‡çš„å…±æ€§é—®é¢˜ï¼ˆTopicï¼‰ã€‚

å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œè¯·æä¾›ï¼š
1. ç®€çŸ­çš„æ ‡é¢˜ï¼ˆå¦‚"ç”µæ± ç»­èˆªä¸è¶³"ã€"å®¢æœå“åº”æ…¢"ï¼‰
2. å±äºè¯¥é—®é¢˜çš„è¯„è®º ID åˆ—è¡¨
3. ä¸€å¥å…¸å‹çš„ç”¨æˆ·åŸè¯æ‘˜è¦

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ï¼š

{{
  "topics": [
    {{
      "topic": "é—®é¢˜æ ‡é¢˜",
      "review_ids": [1, 2, 3],
      "summary": "å…¸å‹ç”¨æˆ·åŸè¯æ‘˜è¦"
    }},
    {{
      "topic": "é—®é¢˜æ ‡é¢˜",
      "review_ids": [4, 5],
      "summary": "å…¸å‹ç”¨æˆ·åŸè¯æ‘˜è¦"
    }}
  ]
}}

ç”¨æˆ·è¯„è®ºï¼š
{reviews}

è¯·ç›´æ¥è¿”å› JSON æ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ï¼š"""
    
    try:
        # è°ƒç”¨ LLM
        prompt = prompt_template.format(reviews=all_reviews)
        response = llm.invoke(prompt)
        
        # æå–å›ç­”
        if hasattr(response, 'content'):
            answer = response.content
        else:
            answer = str(response)
        
        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç å—ï¼‰
        json_str = answer.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if json_str.startswith("```json"):
            json_str = json_str[7:]
        elif json_str.startswith("```"):
            json_str = json_str[3:]
        if json_str.endswith("```"):
            json_str = json_str[:-3]
        json_str = json_str.strip()
        
        # è§£æ JSON
        try:
            result = json.loads(json_str)
            topics = result.get('topics', [])
            
            # éªŒè¯å’Œæ¸…ç†æ•°æ®
            valid_topics = []
            for topic_data in topics:
                if 'topic' in topic_data and 'review_ids' in topic_data:
                    valid_topics.append({
                        'topic': topic_data['topic'],
                        'review_ids': topic_data['review_ids'] if isinstance(topic_data['review_ids'], list) else [],
                        'summary': topic_data.get('summary', '')
                    })
            
            return valid_topics
            
        except json.JSONDecodeError as e:
            st.warning(f"JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            # å°è¯•æå– JSON å¯¹è±¡
            import re
            json_match = re.search(r'\{.*\}', json_str, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    return result.get('topics', [])
                except:
                    pass
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            st.error(f"æ— æ³•è§£æ LLM è¿”å›çš„ JSONã€‚åŸå§‹å“åº”ï¼š\n{answer[:500]}")
            return []
            
    except Exception as e:
        st.error(f"LLM åˆ†æå‡ºé”™: {e}")
        return []


def convert_topics_to_aggregated_format(topics, reviews_df):
    """
    å°† LLM è¿”å›çš„ topics è½¬æ¢ä¸ºèšåˆæ ¼å¼ï¼Œä¾¿äº UI å±•ç¤º
    
    Args:
        topics: LLM è¿”å›çš„ topics åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« topic, review_ids, summary
        reviews_df: åŸå§‹è¯„è®º DataFrameï¼Œç”¨äºæ ¹æ® review_ids åæŸ¥è¯„è®ºå†…å®¹
    
    Returns:
        list: èšåˆåçš„æŠ±æ€¨åˆ—è¡¨ï¼Œæ ¼å¼ä¸åŸæ¥çš„ aggregate_complaints å…¼å®¹
    """
    aggregated = []
    
    # ç¡®ä¿ reviews_df æœ‰ review_id åˆ—
    if 'review_id' not in reviews_df.columns and 'user_id' in reviews_df.columns:
        reviews_df = reviews_df.copy()
        reviews_df['review_id'] = reviews_df['user_id']
    elif 'review_id' not in reviews_df.columns:
        reviews_df = reviews_df.copy()
        reviews_df['review_id'] = range(1, len(reviews_df) + 1)
    
    # æ”¶é›†æ‰€æœ‰å·²å½’ç±»çš„ review_idsï¼Œç”¨äºæ£€æµ‹é‡å¤å’Œé—æ¼
    all_classified_review_ids = set()
    
    for topic_data in topics:
        topic = topic_data.get('topic', 'æœªçŸ¥é—®é¢˜')
        review_ids = topic_data.get('review_ids', [])
        summary = topic_data.get('summary', '')
        
        # å»é‡ï¼šå¦‚æœåŒä¸€ä¸ª review_id å‡ºç°åœ¨å¤šä¸ª topics ä¸­ï¼Œåªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„
        unique_review_ids = []
        for rid in review_ids:
            if rid not in all_classified_review_ids:
                unique_review_ids.append(rid)
                all_classified_review_ids.add(rid)
        
        # å¦‚æœå»é‡åæ²¡æœ‰æœ‰æ•ˆçš„ review_idsï¼Œè·³è¿‡è¿™ä¸ª topic
        if not unique_review_ids:
            continue
        
        # æ ¹æ® review_ids ä» DataFrame ä¸­åæŸ¥è¯„è®ºå†…å®¹
        reviews = []
        for rid in unique_review_ids:
            matching_rows = reviews_df[reviews_df['review_id'] == rid]
            if not matching_rows.empty:
                review_text = matching_rows.iloc[0].get('review_text', '') or matching_rows.iloc[0].get('content', '')
                if review_text:
                    reviews.append(review_text)
        
        aggregated.append({
            'complaint': topic,
            'count': len(unique_review_ids),  # ä½¿ç”¨å»é‡åçš„æ•°é‡
            'reviews': reviews,
            'summary': summary,
            'review_ids': unique_review_ids  # ä¿å­˜å»é‡åçš„ review_ids
        })
    
    # æŒ‰å‡ºç°æ¬¡æ•°é™åºæ’åˆ—
    aggregated.sort(key=lambda x: x['count'], reverse=True)
    
    # éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„è´Ÿé¢è¯„è®º
    all_negative_review_ids = set(reviews_df['review_id'].tolist())
    unclassified_ids = all_negative_review_ids - all_classified_review_ids
    
    if unclassified_ids:
        # å¦‚æœæœ‰æœªå½’ç±»çš„è¯„è®ºï¼Œåˆ›å»ºä¸€ä¸ª"å…¶ä»–é—®é¢˜"ç±»åˆ«ï¼Œç¡®ä¿æ‰€æœ‰è¯„è®ºéƒ½è¢«ç»Ÿè®¡
        unclassified_reviews = []
        for rid in unclassified_ids:
            matching_rows = reviews_df[reviews_df['review_id'] == rid]
            if not matching_rows.empty:
                review_text = matching_rows.iloc[0].get('review_text', '') or matching_rows.iloc[0].get('content', '')
                if review_text:
                    unclassified_reviews.append(review_text)
        
        # åˆ›å»º"å…¶ä»–é—®é¢˜"ç±»åˆ«
        aggregated.append({
            'complaint': 'å…¶ä»–é—®é¢˜',
            'count': len(unclassified_ids),
            'reviews': unclassified_reviews,
            'summary': f'åŒ…å« {len(unclassified_ids)} æ¡æœªæ˜ç¡®å½’ç±»åˆ°ç‰¹å®šé—®é¢˜ç±»å‹çš„è´Ÿé¢è¯„è®º',
            'review_ids': list(unclassified_ids)
        })
        
        # é‡æ–°æ’åºï¼ˆå› ä¸ºæ·»åŠ äº†æ–°é¡¹ï¼‰
        aggregated.sort(key=lambda x: x['count'], reverse=True)
    
    return aggregated


def match_with_spec(complaint, qa_chain=None):
    """å°†ç”¨æˆ·æŠ±æ€¨ä¸äº§å“è¯´æ˜ä¹¦è¿›è¡ŒåŒ¹é…ï¼ˆä½¿ç”¨ RAGï¼‰"""
    
    # å¦‚æœæ²¡æœ‰ RAG é“¾ï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ä½œä¸ºåå¤‡
    if not qa_chain:
        if 'ä¸­æ–‡æ’­å®¢' in complaint or 'ä¸­æ–‡' in complaint:
            spec_match = "éŸ³é¢‘ä¸è¯­è¨€é™åˆ¶ï¼šAudio Overview ç›®å‰å¼ºè°ƒä¸ºå®éªŒæ€§åŠŸèƒ½...ä¸­æ–‡æ’­å®¢å¼è¾“å‡ºä½“éªŒæ˜æ˜¾å¼±äºè‹±æ–‡"
            conclusion = "âœ… äº§å“å·²çŸ¥å±€é™ - è¯´æ˜ä¹¦å·²æ˜ç¡®æ ‡æ³¨ä¸­æ–‡æ”¯æŒæœ‰é™"
        elif 'PDF' in complaint or 'å›¾è¡¨' in complaint:
            spec_match = "å†…å®¹ä¸æ–‡ä»¶é™åˆ¶ï¼šå¯¹çº¯å›¾ç‰‡ PDFã€å¤æ‚è¡¨æ ¼æˆ–å›¾åƒä¿¡æ¯æ”¯æŒæœ‰é™ï¼Œå›¾è¡¨å’Œå›¾åƒå‹ PDF åœ¨è§£æå’Œæ£€ç´¢æ—¶ä»å¯èƒ½ä¸¢å¤±æˆ–å¼±åŒ–"
            conclusion = "âœ… äº§å“å·²çŸ¥å±€é™ - è¯´æ˜ä¹¦å·²æ˜ç¡®æ ‡æ³¨å›¾è¡¨è§£æå—é™"
        else:
            spec_match = "æœªåœ¨è¯´æ˜ä¹¦ä¸­æ‰¾åˆ°å¯¹åº”æè¿°"
            conclusion = "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥ - å¯èƒ½æ˜¯æ–°å‘ç°çš„é—®é¢˜"
        return spec_match, conclusion, []
    
    # ä½¿ç”¨ RAG è¿›è¡ŒçœŸå®æ£€ç´¢å’Œåˆ†æ
    try:
        query = f"ç”¨æˆ·åé¦ˆï¼š{complaint}ã€‚è¯·åˆ†æè¿™æ˜¯äº§å“å·²çŸ¥å±€é™è¿˜æ˜¯æ–°é—®é¢˜ã€‚"
        answer, source_docs = perform_rag_query(qa_chain['vectorstore'], qa_chain['llm'], query)
        
        if not answer:
            raise Exception("RAG æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
        
        # è§£æå›ç­”ï¼Œæå–è¯´æ˜ä¹¦å‚æ•°å’Œç»“è®º
        spec_match = ""
        conclusion = ""
        
        # ä»å›ç­”ä¸­æå–ä¿¡æ¯
        if "è¯´æ˜ä¹¦å¯¹åº”å‚æ•°" in answer:
            parts = answer.split("è¯´æ˜ä¹¦å¯¹åº”å‚æ•°ï¼š")
            if len(parts) > 1:
                spec_part = parts[1].split("AI åˆ¤å®šç»“è®ºï¼š")[0].strip()
                spec_match = spec_part if spec_part else "æœªæ‰¾åˆ°ç›¸å…³è¯´æ˜"
        else:
            spec_match = "æœªåœ¨è¯´æ˜ä¹¦ä¸­æ‰¾åˆ°å¯¹åº”æè¿°"
        
        if "AI åˆ¤å®šç»“è®º" in answer:
            conclusion = answer.split("AI åˆ¤å®šç»“è®ºï¼š")[-1].strip()
        else:
            # ä»å›ç­”ä¸­æ¨æ–­ç»“è®º
            if "å·²çŸ¥å±€é™" in answer or "âœ…" in answer:
                conclusion = "âœ… äº§å“å·²çŸ¥å±€é™ - " + answer[:50]
            elif "æ–°é—®é¢˜" in answer or "âš ï¸" in answer:
                conclusion = "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥ - " + answer[:50]
            else:
                conclusion = "â“ éœ€è¦äººå·¥åˆ¤æ–­ - " + answer[:50]
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œä½¿ç”¨æºæ–‡æ¡£å†…å®¹
        if not spec_match and source_docs:
            spec_match = "\n\n".join([doc.page_content[:200] + "..." for doc in source_docs[:2]])
        
        # è¿”å›æºæ–‡æ¡£å†…å®¹ç”¨äºå±•ç¤ºï¼ˆå»é‡ï¼‰
        source_contents = []
        seen_contents = set()
        for doc in source_docs:
            content = doc.page_content
            # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹å»é‡
            fingerprint = content[:100].strip()
            if fingerprint not in seen_contents:
                seen_contents.add(fingerprint)
                source_contents.append(content)
        
        return spec_match, conclusion, source_contents
        
    except Exception as e:
        st.warning(f"RAG åˆ†æå‡ºé”™: {e}ï¼Œä½¿ç”¨åå¤‡æ–¹æ¡ˆ")
        # åå¤‡æ–¹æ¡ˆ
        if 'ä¸­æ–‡æ’­å®¢' in complaint or 'ä¸­æ–‡' in complaint:
            spec_match = "éŸ³é¢‘ä¸è¯­è¨€é™åˆ¶ï¼šAudio Overview ç›®å‰å¼ºè°ƒä¸ºå®éªŒæ€§åŠŸèƒ½...ä¸­æ–‡æ’­å®¢å¼è¾“å‡ºä½“éªŒæ˜æ˜¾å¼±äºè‹±æ–‡"
            conclusion = "âœ… äº§å“å·²çŸ¥å±€é™ - è¯´æ˜ä¹¦å·²æ˜ç¡®æ ‡æ³¨ä¸­æ–‡æ”¯æŒæœ‰é™"
        elif 'PDF' in complaint or 'å›¾è¡¨' in complaint:
            spec_match = "å†…å®¹ä¸æ–‡ä»¶é™åˆ¶ï¼šå¯¹çº¯å›¾ç‰‡ PDFã€å¤æ‚è¡¨æ ¼æˆ–å›¾åƒä¿¡æ¯æ”¯æŒæœ‰é™ï¼Œå›¾è¡¨å’Œå›¾åƒå‹ PDF åœ¨è§£æå’Œæ£€ç´¢æ—¶ä»å¯èƒ½ä¸¢å¤±æˆ–å¼±åŒ–"
            conclusion = "âœ… äº§å“å·²çŸ¥å±€é™ - è¯´æ˜ä¹¦å·²æ˜ç¡®æ ‡æ³¨å›¾è¡¨è§£æå—é™"
        else:
            spec_match = "æœªåœ¨è¯´æ˜ä¹¦ä¸­æ‰¾åˆ°å¯¹åº”æè¿°"
            conclusion = "âš ï¸ éœ€è¿›ä¸€æ­¥è°ƒæŸ¥ - å¯èƒ½æ˜¯æ–°å‘ç°çš„é—®é¢˜"
        return spec_match, conclusion, []


def generate_ai_brief(df, negative_ratio):
    """ç”Ÿæˆ AI æ¯æ—¥ç®€æŠ¥ï¼ˆåŸºäºå®é™…ç”¨æˆ·åé¦ˆæ•°æ®ï¼‰"""
    negative_count = len(df[df['rating'] < 3])
    positive_count = len(df[df['rating'] >= 4])
    
    # å¦‚æœå·²æœ‰åˆ†æç»“æœï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é€šç”¨æè¿°
    if 'analysis_topics' in st.session_state:
        topics = st.session_state['analysis_topics']
        top_issues = [t.get('topic', '') for t in topics[:3]]
        top_issue_text = "ã€".join([f"**{issue}**" for issue in top_issues[:2] if issue])
    else:
        top_issue_text = "åŠŸèƒ½ä½¿ç”¨é—®é¢˜"
    
    brief = f"""
### ğŸ“Š èˆ†æƒ…è¶‹åŠ¿åˆ†æ

**æ•´ä½“æƒ…ç»ªï¼š** {"ğŸ˜Š æ­£å‘ä¸ºä¸»" if negative_ratio < 30 else "ğŸ˜ ä¸­æ€§åè´Ÿ" if negative_ratio < 50 else "ğŸ˜Ÿ è´Ÿå‘é¢„è­¦"}

**æ ¸å¿ƒå‘ç°ï¼š**
- æœ¬å‘¨å…±æ”¶é›† **{len(df)}** æ¡ç”¨æˆ·åé¦ˆï¼Œå…¶ä¸­æ­£å‘è¯„ä»· **{positive_count}** æ¡ï¼Œè´Ÿå‘è¯„ä»· **{negative_count}** æ¡
- ç”¨æˆ·åé¦ˆä¸»è¦é›†ä¸­åœ¨ **äº§å“åŠŸèƒ½é™åˆ¶è¯´æ˜ä¸æ¸…** å’Œ **å®é™…æ€§èƒ½ä¸å®£ä¼ å‚æ•°ä¸ç¬¦** ä¸¤å¤§æ–¹é¢
- å½“å‰æœ€çªå‡ºçš„é—®é¢˜ç±»å‹ï¼š{top_issue_text if top_issue_text else "åŠŸèƒ½ä½¿ç”¨é—®é¢˜"}

**èˆ†æƒ…é¢„è­¦ï¼š**
- ğŸ”´ æ–°æ‰‹ç”¨æˆ·å¯¹äº§å“é™åˆ¶æ¡ä»¶çš„è®¤çŸ¥ä¸è¶³ï¼Œå¯¼è‡´ä½¿ç”¨ä½“éªŒå·®
- ğŸŸ¡ ç¡¬ä»¶è´¨é‡é—®é¢˜å½±å“ç”¨æˆ·å¯¹äº§å“å“è´¨çš„ä¿¡ä»»
- ğŸŸ¡ è¯´æ˜ä¹¦å¯è¯»æ€§ä¸è¶³ï¼Œç”¨æˆ·éš¾ä»¥å¿«é€Ÿç†è§£å…³é”®é™åˆ¶æ¡ä»¶

**å»ºè®®å…³æ³¨ï¼š**
- ä¼˜åŒ–äº§å“è¯´æ˜ä¹¦ï¼Œçªå‡ºå…³é”®é™åˆ¶æ¡ä»¶
- åŠ å¼ºæ–°æ‰‹å¼•å¯¼ï¼Œåœ¨äº§å“é¦–æ¬¡ä½¿ç”¨æ—¶ä¸»åŠ¨æç¤ºé‡è¦é™åˆ¶
- å…³æ³¨ç¡¬ä»¶å“æ§ï¼Œå‡å°‘è´¨é‡é—®é¢˜
"""
    return brief


# ==================== Action Plan æ•°æ®ç»“æ„ ====================
class ActionType(str, Enum):
    """è¡ŒåŠ¨ç±»å‹æšä¸¾"""
    JIRA_TICKET = "Jira Ticket"
    DOC_UPDATE = "Doc Update"
    EMAIL_DRAFT = "Email Draft"
    MEETING = "Meeting"


class Priority(str, Enum):
    """ä¼˜å…ˆçº§æšä¸¾"""
    HIGH = "High"
    MEDIUM = "Medium"
    LOW = "Low"


if PYDANTIC_AVAILABLE:
    class ActionPlan(BaseModel):
        """è¡ŒåŠ¨è®¡åˆ’æ•°æ®æ¨¡å‹"""
        action_type: ActionType
        title: str
        content: str
        priority: Priority
        
        class Config:
            use_enum_values = True
else:
    # å¦‚æœæ²¡æœ‰ Pydanticï¼Œä½¿ç”¨å­—å…¸ç»“æ„
    ActionPlan = dict


def generate_action_plan(topic_name: str, rag_conclusion: str, user_complaints: list, llm) -> Optional[dict]:
    """
    ä½¿ç”¨ LLM æ ¹æ® RAG åˆ†æç»“æœåŠ¨æ€ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’
    
    Args:
        topic_name: é—®é¢˜èšç±»åç§°
        rag_conclusion: RAG åˆ†æå‡ºçš„å½’å› ç»“è®º
        user_complaints: å…¸å‹ç”¨æˆ·åŸè¯åˆ—è¡¨
        llm: LangChain LLM å®ä¾‹
    
    Returns:
        dict: ActionPlan å¯¹è±¡ï¼ˆåŒ…å« action_type, title, content, priorityï¼‰
    """
    if not llm:
        return None
    
    # æ„å»ºç”¨æˆ·æŠ±æ€¨æ‘˜è¦
    complaints_text = "\n".join([f"- {complaint}" for complaint in user_complaints[:5]])
    
    # æ„å»º Prompt
    prompt_template = """ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®é—®é¢˜æ€§è´¨åšå‡ºå†³ç­–çš„äº§å“ç»ç†ã€‚

è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’ï¼š

**é—®é¢˜ç±»å‹ï¼š** {topic_name}

**RAG å½’å› ç»“è®ºï¼š** {rag_conclusion}

**å…¸å‹ç”¨æˆ·åé¦ˆï¼š**
{complaints}

**å†³ç­–è§„åˆ™ï¼š**
- å¦‚æœå½’å› æ˜¯ **äº§å“ç¼ºé™·/Bug** -> ç”Ÿæˆ Jira Ticketï¼ŒåŒ…å«æ ‡é¢˜ã€æè¿°ã€å¤ç°æ­¥éª¤ã€ä¼˜å…ˆçº§
- å¦‚æœå½’å› æ˜¯ **ç”¨æˆ·è¯¯æ“ä½œ/æ–‡æ¡£ä¸æ¸…** -> ç”Ÿæˆ Doc Updateï¼ˆæ›´æ–°è¯´æ˜ä¹¦ï¼‰æˆ– Email Draftï¼ˆå®¢æœè¯æœ¯ï¼‰
- å¦‚æœå½’å› æ˜¯ **ç‰©æµ/æœåŠ¡é—®é¢˜** -> ç”Ÿæˆ Email Draftï¼ˆç»™ç‰©æµå•†æˆ–å®¢æœä¸»ç®¡ï¼‰
- å¦‚æœå½’å› æ˜¯ **å¤æ‚é—®é¢˜éœ€è¦è®¨è®º** -> ç”Ÿæˆ Meetingï¼ˆä¼šè®®å®‰æ’ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ï¼š

{{
  "action_type": "Jira Ticket" | "Doc Update" | "Email Draft" | "Meeting",
  "title": "è¡ŒåŠ¨çš„ç®€çŸ­æ ‡é¢˜ï¼ˆå¦‚ï¼šåˆ›å»º P0 çº§ Jira å·¥å•ï¼šä¿®å¤äº‘å°æŠ–åŠ¨ï¼‰",
  "content": "è¡ŒåŠ¨çš„è¯¦ç»†å†…å®¹ï¼ˆå¦‚å·¥å•çš„ Description æˆ–é‚®ä»¶çš„æ­£æ–‡ï¼Œè¦å…·ä½“å¯æ‰§è¡Œï¼‰",
  "priority": "High" | "Medium" | "Low"
}}

è¯·ç›´æ¥è¿”å› JSON æ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ï¼š"""
    
    try:
        prompt = prompt_template.format(
            topic_name=topic_name,
            rag_conclusion=rag_conclusion,
            complaints=complaints_text
        )
        
        response = llm.invoke(prompt)
        
        # æå–å›ç­”
        if hasattr(response, 'content'):
            answer = response.content
        else:
            answer = str(response)
        
        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç å—ï¼‰
        json_str = answer.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if json_str.startswith("```json"):
            json_str = json_str[7:]
        elif json_str.startswith("```"):
            json_str = json_str[3:]
        if json_str.endswith("```"):
            json_str = json_str[:-3]
        json_str = json_str.strip()
        
        # è§£æ JSON
        try:
            result = json.loads(json_str)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ['action_type', 'title', 'content', 'priority']
            if all(field in result for field in required_fields):
                # éªŒè¯ action_type
                valid_types = ['Jira Ticket', 'Doc Update', 'Email Draft', 'Meeting']
                if result['action_type'] not in valid_types:
                    result['action_type'] = 'Doc Update'  # é»˜è®¤å€¼
                
                # éªŒè¯ priority
                valid_priorities = ['High', 'Medium', 'Low']
                if result['priority'] not in valid_priorities:
                    result['priority'] = 'Medium'  # é»˜è®¤å€¼
                
                return result
            else:
                st.warning(f"LLM è¿”å›çš„ JSON ç¼ºå°‘å¿…éœ€å­—æ®µã€‚åŸå§‹å“åº”ï¼š\n{answer[:500]}")
                return None
                
        except json.JSONDecodeError as e:
            st.warning(f"JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            # å°è¯•æå– JSON å¯¹è±¡
            json_match = re.search(r'\{.*\}', json_str, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    return result
                except:
                    pass
            
            st.error(f"æ— æ³•è§£æ LLM è¿”å›çš„ JSONã€‚åŸå§‹å“åº”ï¼š\n{answer[:500]}")
            return None
            
    except Exception as e:
        st.error(f"ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’æ—¶å‡ºé”™: {e}")
        return None


# ==================== ä¾§è¾¹æ  ====================
with st.sidebar:
    st.markdown("## ğŸ”¬ ReviewOps")
    st.markdown("*ç”¨æˆ·åé¦ˆå†³ç­–ä¸­å°*")
    
    st.divider()
    
    # äº§å“ä¿¡æ¯
    product_name = extract_product_name()
    st.markdown("### ğŸ“¦ å½“å‰åˆ†æäº§å“")
    st.info(f"**{product_name}**\n\näº§å“è¯´æ˜ä¹¦å·²å‘é‡åŒ–å­˜å‚¨")
    
    st.divider()
    
    # API Key è¾“å…¥
    st.markdown("### ğŸ”‘ API é…ç½®")
    
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
    env_api_key = os.getenv("DASHSCOPE_API_KEY", "")
    default_api_key = env_api_key if env_api_key else ""
    
    # å¦‚æœç¯å¢ƒå˜é‡ä¸­æœ‰ï¼Œæ˜¾ç¤ºæç¤ºï¼›å¦åˆ™å…è®¸ç”¨æˆ·è¾“å…¥
    if env_api_key:
        st.info("âœ… å·²ä»ç¯å¢ƒå˜é‡ `DASHSCOPE_API_KEY` è¯»å– API Key")
        api_key = env_api_key
    else:
        api_key = st.text_input(
            "DashScope API Key (é˜¿é‡Œåƒé—®)",
            type="password",
            value="",
            placeholder="sk-... æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY",
            help="ç”¨äº RAG æ·±åº¦åˆ†æåŠŸèƒ½ã€‚æ¨èæ–¹å¼ï¼šåœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œæ·»åŠ  DASHSCOPE_API_KEY=your-key"
        )
        
        if api_key:
            st.success("âœ… API Key å·²é…ç½®ï¼ˆä¸´æ—¶ï¼Œä»…æœ¬æ¬¡ä¼šè¯æœ‰æ•ˆï¼‰")
        else:
            st.warning("âš ï¸ è¯·é…ç½® API Key ä»¥å¯ç”¨ RAG åˆ†æåŠŸèƒ½")
    
    st.divider()
    
    # æ•°æ®æ¦‚è§ˆ
    st.markdown("### ğŸ“Š æ•°æ®æº")
    st.caption(f"ğŸ“„ è¯„è®ºæ•°æ®: `user_reviews.csv`")
    st.caption(f"ğŸ“‹ äº§å“è¯´æ˜: `dji_spec.pdf` (å·²å‘é‡åŒ–)")
    st.caption(f"ğŸ’¾ å‘é‡åº“: `./chroma_db`")
    st.caption(f"ğŸ• æœ€åæ›´æ–°: 2025-01-15")


# ==================== ä¸»ç•Œé¢ ====================
# æ ‡é¢˜åŒº
st.markdown('<h1 class="main-title">ReviewOps</h1>', unsafe_allow_html=True)
st.markdown("**ç”¨æˆ·åé¦ˆå†³ç­–ä¸­å°** Â· è®©äº§å“å†³ç­–æœ‰æ®å¯ä¾")
st.markdown("---")

# ==================== å…¨å±€çŠ¶æ€åˆå§‹åŒ– (SSOT) ====================
# æ£€æŸ¥å¹¶åˆå§‹åŒ– all_reviewsï¼ˆSingle Source of Truthï¼‰
if 'all_reviews' not in st.session_state:
    # åˆå§‹åŒ–ï¼šä» CSV æ–‡ä»¶åŠ è½½å†å²æ•°æ®
    st.session_state.all_reviews = reviews_df.to_dict('records')
    st.session_state.last_run_increment = 0

# åˆå§‹åŒ– RAG åˆ†æç»“æœå­˜å‚¨
if 'latest_rag_results' not in st.session_state:
    st.session_state.latest_rag_results = []

# æ£€æŸ¥æ˜¯å¦éœ€è¦å»¶è¿Ÿåˆ·æ–°é¡µé¢ï¼ˆè®©ç”¨æˆ·å…ˆçœ‹åˆ°å·¥ä½œæµç»“æœï¼‰
if st.session_state.get('need_refresh', False):
    st.session_state['need_refresh'] = False
    # å»¶è¿Ÿåˆ·æ–°ï¼Œè®©ç”¨æˆ·æœ‰æ—¶é—´çœ‹æ¸…å®Œæˆæç¤º
    time.sleep(1)
    st.rerun()

# ==================== é¡¶éƒ¨ Dashboard ====================
st.markdown("## ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")

# è®¡ç®—æŒ‡æ ‡ - åŸºäº session_state.all_reviewsï¼ˆSSOTï¼‰
all_reviews_df = pd.DataFrame(st.session_state.all_reviews)
total_reviews, avg_rating, negative_ratio = calculate_metrics(all_reviews_df)

# ä¸‰ä¸ªæŒ‡æ ‡å¡ç‰‡
col1, col2, col3 = st.columns(3)

with col1:
    # åŠ¨æ€æ˜¾ç¤ºå¢é‡ï¼ˆåŸºäº last_run_incrementï¼‰
    delta_text = f"æœ¬æ¬¡æ–°å¢ {st.session_state.last_run_increment} æ¡" if st.session_state.last_run_increment > 0 else None
    st.metric(
        label="ğŸ“ æ€»è¯„è®ºæ•°",
        value=f"{total_reviews}",
        delta=delta_text,
        delta_color="normal"
    )

with col2:
    st.metric(
        label="â­ å¹³å‡è¯„åˆ†",
        value=f"{avg_rating:.1f}",
        delta="+0.2 vs ä¸Šå‘¨",
        delta_color="normal"
    )

with col3:
    st.metric(
        label="ğŸ˜” è´Ÿé¢è¯„ä»·å æ¯”",
        value=f"{negative_ratio:.1f}%",
        delta="-5% vs ä¸Šå‘¨",
        delta_color="inverse"
    )

st.markdown("")

# AI æ¯æ—¥ç®€æŠ¥
with st.expander("ğŸ¤– **AI æ¯æ—¥ç®€æŠ¥** - ç‚¹å‡»å±•å¼€", expanded=True):
    ai_brief = generate_ai_brief(all_reviews_df, negative_ratio)
    st.markdown(ai_brief)

st.markdown("---")

# ==================== æ™ºèƒ½å·¥ä½œæµåŒº ====================
st.markdown("## âš¡ æ™ºèƒ½å·¥ä½œæµ")
st.caption("åŸºäº LangGraph çš„è‡ªåŠ¨åŒ–å·¡æ£€ç³»ç»Ÿï¼Œè‡ªåŠ¨ç›‘æ§ã€ç­›é€‰ã€åˆ†æå’Œç”Ÿæˆè¡ŒåŠ¨å»ºè®®")

# å·¥ä½œæµæŒ‰é’®
col_workflow, col_manual, col_space = st.columns([1, 1, 2])
with col_workflow:
    workflow_button = st.button("âš¡ è¿è¡Œæ™ºèƒ½å·¥ä½œæµ", use_container_width=True, type="primary")
with col_manual:
    analyze_button = st.button("ğŸš€ æ‰‹åŠ¨å½’å› åˆ†æ", use_container_width=True)

# ==================== æ™ºèƒ½å·¥ä½œæµæ‰§è¡Œ ====================
if workflow_button:
    # æ£€æŸ¥ API Key
    if not api_key:
        st.error("âŒ è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® DashScope API Key")
        st.stop()
    
    try:
        # å¯¼å…¥å·¥ä½œæµ
        from agent_graph import graph_app
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "raw_reviews": [],
            "critical_reviews": [],
            "rag_analysis_results": [],
            "action_plans": [],
            "logs": []
        }
        
        # ä½¿ç”¨ st.status å±•ç¤ºå®æ—¶æ—¥å¿—
        with st.status("ğŸ”„ å·¥ä½œæµè¿è¡Œä¸­...", expanded=True) as status:
            st.write("ğŸš€ å¯åŠ¨æ™ºèƒ½å·¥ä½œæµ...")
            
            # æ•°æ®åŒæ­¥ï¼šä½¿ç”¨ stream() ç›‘å¬æµå¼è¾“å‡º
            final_state = initial_state.copy()
            for event in graph_app.stream(initial_state):
                # éå†æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º
                for node_name, node_output in event.items():
                    # åˆå¹¶çŠ¶æ€
                    if isinstance(node_output, dict):
                        final_state.update(node_output)
                    
                    # æ£€æµ‹ node_monitor äº§å‡ºçš„ raw_reviews
                    if node_name == "monitor" and isinstance(node_output, dict) and "raw_reviews" in node_output:
                        new_reviews = node_output.get("raw_reviews", [])
                        if new_reviews:
                            # æ•°æ®åŒæ­¥ï¼šç«‹å³è¿½åŠ åˆ° session_state.all_reviews
                            st.session_state.all_reviews.extend(new_reviews)
                            st.session_state.last_run_increment = len(new_reviews)
                            st.write(f"ğŸ“¥ æ•°æ®åŒæ­¥ï¼šå·²æ·»åŠ  {len(new_reviews)} æ¡æ–°è¯„è®ºåˆ°å…¨å±€çŠ¶æ€")
                    
                    # æ£€æµ‹ node_rag_analysis äº§å‡ºçš„ rag_analysis_results
                    if node_name == "rag_analysis" and isinstance(node_output, dict) and "rag_analysis_results" in node_output:
                        rag_results = node_output.get("rag_analysis_results", [])
                        if rag_results:
                            # ä¿å­˜ RAG åˆ†æç»“æœåˆ° session_stateï¼Œé˜²æ­¢é¡µé¢åˆ·æ–°åä¸¢å¤±
                            st.session_state.latest_rag_results = rag_results
                            st.write(f"ğŸ“„ RAG åˆ†æç»“æœå·²ä¿å­˜ï¼š{len(rag_results)} æ¡")
                    
                    # å®æ—¶æ˜¾ç¤ºæ—¥å¿—
                    if isinstance(node_output, dict) and "logs" in node_output:
                        logs = node_output.get("logs", [])
                        for log in logs:
                            st.write(log)
                            time.sleep(0.2)  # æ¨¡æ‹Ÿå®æ—¶æ›´æ–°
            
            status.update(label="âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ", state="complete")
            
            # å¼ºåˆ¶åˆ·æ–°ï¼šåœ¨å·¥ä½œæµè¿è¡Œå®Œæ¯•ã€æ—¥å¿—æ˜¾ç¤º"âœ… å®Œæˆ"åï¼Œæ·»åŠ å»¶è¿Ÿç„¶ååˆ·æ–°
            st.write("â³ æ­£åœ¨åˆ·æ–°é¡µé¢ä»¥æ›´æ–°ç»Ÿè®¡æ•°æ®...")
            time.sleep(1)
            
            # æ ‡è®°éœ€è¦åˆ·æ–°ï¼Œä½†ä¸åœ¨è¿™é‡Œç›´æ¥è°ƒç”¨ st.rerun()ï¼ˆå› ä¸ºè¿˜åœ¨ status å®¹å™¨å†…ï¼‰
            st.session_state['need_refresh'] = True
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        st.success(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
        
        # ä½¿ç”¨æœ€ç»ˆçŠ¶æ€
        result = final_state
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“¥ æ–°è¯„è®º", len(result.get("raw_reviews", [])))
        with col2:
            st.metric("ğŸ” é«˜å±è¯„è®º", len(result.get("critical_reviews", [])))
        with col3:
            st.metric("ğŸ“„ å½’å› ç»“æœ", len(result.get("rag_analysis_results", [])))
        with col4:
            st.metric("ğŸ’¡ è¡ŒåŠ¨å»ºè®®", len(result.get("action_plans", [])))
        
        # æ˜¾ç¤ºè¡ŒåŠ¨å»ºè®®å¡ç‰‡
        action_plans = result.get("action_plans", [])
        if action_plans:
            st.markdown("---")
            st.markdown("### ğŸ’¡ ç”Ÿæˆçš„è¡ŒåŠ¨å»ºè®®")
            
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            priority_order = {"High": 3, "Medium": 2, "Low": 1}
            sorted_actions = sorted(
                action_plans,
                key=lambda x: (priority_order.get(x.get("priority", "Medium"), 2), x.get("title", "")),
                reverse=True
            )
            
            for idx, action in enumerate(sorted_actions, 1):
                action_type = action.get("action_type", "Jira Ticket")
                title = action.get("title", "")
                content = action.get("content", "")
                priority = action.get("priority", "Medium")
                
                # ä¼˜å…ˆçº§é¢œè‰²
                priority_colors = {
                    "High": "ğŸ”´",
                    "Medium": "ğŸŸ¡",
                    "Low": "ğŸŸ¢"
                }
                priority_icon = priority_colors.get(priority, "ğŸŸ¡")
                
                # è¡ŒåŠ¨ç±»å‹å›¾æ ‡
                type_icons = {
                    "Jira Ticket": "ğŸ",
                    "Doc Update": "ğŸ“",
                    "Email Draft": "ğŸ“§",
                    "Meeting": "ğŸ“…"
                }
                type_icon = type_icons.get(action_type, "ğŸ“‹")
                
                with st.expander(f"{type_icon} **{title}** Â· {priority_icon} {priority} Â· {action_type}", expanded=(idx <= 2)):
                    st.markdown(f"**ä¼˜å…ˆçº§ï¼š** {priority}")
                    st.markdown(f"**ç±»å‹ï¼š** {action_type}")
                    st.markdown(f"**å†…å®¹ï¼š**")
                    if len(content) > 500:
                        st.text_area("", value=content, height=150, disabled=True, key=f"action_content_{idx}", label_visibility="collapsed")
                    else:
                        st.markdown(content)
                    
                    # Mock æŒ‰é’®
                    if action_type == "Jira Ticket":
                        if st.button("ğŸš€ æ¨é€è‡³ Jira", key=f"workflow_jira_{idx}", use_container_width=True):
                            ticket_id = f"DJI-2025-{1000 + idx}"
                            st.toast(f"âœ… å·¥å•å·²åˆ›å»ºï¼Ticket ID: {ticket_id}", icon="ğŸ‰")
                    elif action_type == "Doc Update":
                        if st.button("ğŸ“ åˆ›å»º Notion Task", key=f"workflow_notion_{idx}", use_container_width=True):
                            st.toast("âœ… Notion ä»»åŠ¡å·²åˆ›å»ºï¼", icon="ğŸ‰")
                    elif action_type == "Email Draft":
                        if st.button("ğŸ“§ å¤åˆ¶é‚®ä»¶", key=f"workflow_email_{idx}", use_container_width=True):
                            st.toast("âœ… é‚®ä»¶å†…å®¹å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼", icon="ğŸ‰")
                    elif action_type == "Meeting":
                        if st.button("ğŸ“… åˆ›å»ºä¼šè®®", key=f"workflow_meeting_{idx}", use_container_width=True):
                            st.toast("âœ… ä¼šè®®å·²åˆ›å»ºï¼", icon="ğŸ‰")
        
        # å­˜å‚¨ç»“æœåˆ° session_state
        st.session_state['workflow_result'] = result
        st.session_state['workflow_completed'] = True
        
        # æ ‡è®°éœ€è¦åˆ·æ–°é¡µé¢ï¼ˆä½†ä¸ç«‹å³åˆ·æ–°ï¼Œè®©ç”¨æˆ·å…ˆçœ‹åˆ°ç»“æœï¼‰
        st.session_state['need_refresh'] = True
        
    except ImportError as e:
        st.error(f"âŒ æ— æ³•å¯¼å…¥å·¥ä½œæµæ¨¡å—: {e}")
        st.info("ğŸ’¡ è¯·ç¡®ä¿ `agent_graph.py` æ–‡ä»¶å­˜åœ¨ä¸”å·²æ­£ç¡®é…ç½®")
    except Exception as e:
        st.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        st.exception(e)

# ==================== ä¸­é—´æ ¸å¿ƒåŒºï¼šRAG åˆ†æ ====================
st.markdown("## ğŸ” RAG å½’å› åˆ†æ")
st.caption("åŸºäºäº§å“è¯´æ˜ä¹¦å¯¹ç”¨æˆ·åé¦ˆè¿›è¡Œæ™ºèƒ½å½’å› ï¼Œè¯†åˆ«é—®é¢˜æ ¹æº")

# æ˜¾ç¤ºå·¥ä½œæµç”Ÿæˆçš„ RAG åˆ†æç»“æœ
workflow_rag_results = st.session_state.get('latest_rag_results', [])
if workflow_rag_results:
    st.info(f"ğŸ“Š å·¥ä½œæµå·²ç”Ÿæˆ {len(workflow_rag_results)} æ¡ RAG å½’å› åˆ†æç»“æœ")
    
    for idx, rag_result in enumerate(workflow_rag_results, 1):
        review_id = rag_result.get("review_id", f"æœªçŸ¥_{idx}")
        review_text = rag_result.get("review_text", "")
        conclusion = rag_result.get("conclusion", "â“ éœ€è¦äººå·¥åˆ¤æ–­")
        reason = rag_result.get("reason", "")
        
        # æ ¹æ®ç»“è®ºç±»å‹è®¾ç½®é¢œè‰²å’Œå›¾æ ‡
        if "âœ…" in conclusion or "äº§å“å·²çŸ¥å±€é™" in conclusion:
            color = "ğŸŸ¢"
            conclusion_type = "äº§å“å·²çŸ¥å±€é™"
        elif "âš ï¸" in conclusion or "éœ€è¿›ä¸€æ­¥è°ƒæŸ¥" in conclusion:
            color = "ğŸŸ¡"
            conclusion_type = "éœ€è¿›ä¸€æ­¥è°ƒæŸ¥"
        else:
            color = "ğŸ”µ"
            conclusion_type = "ç”¨æˆ·ä½¿ç”¨é—®é¢˜"
        
        # æå–é—®é¢˜æ ‡é¢˜ï¼ˆä»è¯„è®ºä¸­æå–å…³é”®è¯ï¼‰
        title_keywords = ["ç»­èˆª", "é¿éšœ", "äº‘å°", "æŠ–åŠ¨", "ç”µæ± ", "å›¾ä¼ ", "GPS", "è™šæ ‡"]
        title = "æœªçŸ¥é—®é¢˜"
        for keyword in title_keywords:
            if keyword in review_text:
                title = keyword + "ç›¸å…³é—®é¢˜"
                break
        
        with st.expander(f"{color} **{conclusion_type}** Â· {title} (ID: {review_id})", expanded=(idx == 1)):
            col_left, col_mid, col_right = st.columns([1, 1, 1])
            
            with col_left:
                st.markdown("##### ğŸ’¬ ç”¨æˆ·åŸè¯")
                st.info(review_text)
            
            with col_mid:
                st.markdown("##### ğŸ“– RAG è¯æ®")
                # è¿™é‡Œæš‚æ—¶æ˜¾ç¤ºå ä½æ–‡æœ¬ï¼Œåç»­å¯ä»¥æ¥å…¥çœŸå®çš„å‘é‡æ£€ç´¢ç»“æœ
                st.warning("âš ï¸ å½“å‰ä½¿ç”¨åŸºç¡€ RAG é€»è¾‘ï¼Œæœªæ¥å…¥å‘é‡æ£€ç´¢ã€‚\n\nåç»­ç‰ˆæœ¬å°†æ˜¾ç¤ºä»äº§å“è¯´æ˜ä¹¦ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³è¯æ®ç‰‡æ®µã€‚")
            
            with col_right:
                st.markdown("##### ğŸ¤– AI åˆ¤å®š")
                st.markdown(f"**ç»“è®ºï¼š** {conclusion}")
                st.markdown(f"**åˆ†æï¼š** {reason if reason else 'æš‚æ— è¯¦ç»†åˆ†æ'}")
        
        if idx < len(workflow_rag_results):
            st.divider()
elif st.session_state.get('workflow_completed', False):
    st.info("ğŸ’¡ å·¥ä½œæµå·²å®Œæˆï¼Œä½†æœªç”Ÿæˆ RAG åˆ†æç»“æœï¼ˆå¯èƒ½å› ä¸ºæ— é«˜å±è¯„è®ºï¼‰")

st.markdown("---")

if analyze_button:
    # æ£€æŸ¥ API Key
    if not api_key:
        st.error("âŒ è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® DashScope API Key")
        st.stop()
    
    # åˆå§‹åŒ– RAG ç»„ä»¶
    with st.spinner("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ..."):
        vectorstore = init_vectorstore(api_key)
        llm = init_llm(api_key)
        
        if not vectorstore or not llm:
            st.error("âŒ RAG ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key å’Œå‘é‡åº“")
            st.stop()
        
        # å­˜å‚¨ RAG ç»„ä»¶åˆ° session_state
        st.session_state['vectorstore'] = vectorstore
        st.session_state['llm'] = llm
    
    # AI åˆ†æè¿‡ç¨‹
    with st.spinner("ğŸ§  AI æ­£åœ¨åˆ†æä¸­..."):
        progress_bar = st.progress(0)
        
        # Step 1
        st.toast("ğŸ“¥ æ­£åœ¨æå–è´Ÿé¢è¯„ä»·...")
        negative_reviews = get_negative_reviews(reviews_df)
        # å­˜å‚¨è´Ÿé¢è¯„è®ºæ€»æ•°ï¼Œä¾›åç»­æ˜¾ç¤ºä½¿ç”¨
        st.session_state['total_negative_reviews'] = len(negative_reviews)
        time.sleep(0.3)
        progress_bar.progress(25)
        
        # Step 2: ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰èšç±»
        st.toast("ğŸ¤– AI æ­£åœ¨åˆ†æç”¨æˆ·åé¦ˆå¹¶è‡ªåŠ¨èšç±»...")
        topics = analyze_reviews_with_llm(negative_reviews, llm)
        time.sleep(0.3)
        progress_bar.progress(50)
        
        if not topics:
            st.error("âŒ LLM åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key å’Œç½‘ç»œè¿æ¥")
            st.stop()
        
        # Step 3
        st.toast("ğŸ“„ æ­£åœ¨åŒ¹é…äº§å“è¯´æ˜ä¹¦ï¼ˆRAG æ£€ç´¢ï¼‰...")
        # è¿™é‡Œä¼šç¨ååœ¨æ˜¾ç¤ºç»“æœæ—¶è¿›è¡Œ RAG åˆ†æ
        time.sleep(0.3)
        progress_bar.progress(75)
        
        # Step 4
        st.toast("ğŸ’¡ æ­£åœ¨ç”Ÿæˆåˆ†æç»“è®º...")
        time.sleep(0.3)
        progress_bar.progress(100)
    
    st.success("âœ… åˆ†æå®Œæˆï¼")
    
    # å°† LLM è¿”å›çš„ topics è½¬æ¢ä¸ºèšåˆæ ¼å¼
    aggregated_complaints = convert_topics_to_aggregated_format(topics, negative_reviews)
    
    # å­˜å‚¨åˆ° session_state ä¾› Action éƒ¨åˆ†ä½¿ç”¨
    st.session_state['analysis_topics'] = topics
    st.session_state['analysis_results'] = aggregated_complaints  # å…¼å®¹æ—§ä»£ç 
    st.session_state['aggregated_complaints'] = aggregated_complaints
    
    # åˆå§‹åŒ–è¿‡æ»¤å™¨çŠ¶æ€
    if 'selected_complaint_filter' not in st.session_state:
        st.session_state['selected_complaint_filter'] = None

# ===== æ˜¾ç¤ºåˆ†æç»“æœï¼ˆç‹¬ç«‹äºæŒ‰é’®ç‚¹å‡»ï¼Œä¾¿äºè¿‡æ»¤ååˆ·æ–°ï¼‰ =====
if 'aggregated_complaints' in st.session_state:
    aggregated_complaints = st.session_state['aggregated_complaints']
    # å…¼å®¹æ—§ä»£ç ï¼šanalysis_results å¯èƒ½æ˜¯æ—§çš„æ ¼å¼ï¼Œä¹Ÿå¯èƒ½æ˜¯æ–°çš„æ ¼å¼
    complaints = st.session_state.get('analysis_results', aggregated_complaints)
    
    # é—®é¢˜åˆ†å¸ƒç»Ÿè®¡ - ç§»åˆ°å½’å› å¡ç‰‡ä¸Šæ–¹ï¼Œä¾¿äºäº¤äº’
    st.markdown("### ğŸ“Š é—®é¢˜åˆ†å¸ƒ")
    st.caption("ğŸ’¡ ç‚¹å‡»å›¾è¡¨æ‰‡åŒºå¯è¿‡æ»¤ä¸‹æ–¹çš„å½’å› å¡ç‰‡")
    
    # æ„å»ºç»Ÿè®¡æ•°æ® - æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼ˆå‡ºç°æ¬¡æ•°ä»é«˜åˆ°ä½ï¼‰
    sorted_complaints = sorted(aggregated_complaints, key=lambda x: x['count'], reverse=True)
    
    complaint_counts = pd.DataFrame([
        {'é—®é¢˜ç±»å‹': agg['complaint'], 'å‡ºç°æ¬¡æ•°': agg['count']} 
        for agg in sorted_complaints
    ])
    
    # ç”Ÿæˆæ¸å˜é¢œè‰²ï¼ˆä»æ·±çº¢åˆ°æµ…çº¢ï¼Œè¡¨ç¤ºä¸¥é‡ç¨‹åº¦ï¼‰
    n_issues = len(sorted_complaints)
    if n_issues == 1:
        colors = ['#dc2626']  # æ·±çº¢
    else:
        # ä»æ·±çº¢åˆ°æµ…æ©™çš„æ¸å˜
        color_scale = ['#dc2626', '#ef4444', '#f87171', '#fca5a5', '#fed7aa', '#fef3c7']
        colors = color_scale[:n_issues] if n_issues <= len(color_scale) else color_scale
    
    col_chart, col_insight = st.columns([2, 1])
    
    # è·å–æ€»è´Ÿé¢è¯„è®ºæ•°ï¼Œç”¨äºè®¡ç®—ç™¾åˆ†æ¯”
    total_negative_count = st.session_state.get('total_negative_reviews', sum(agg['count'] for agg in sorted_complaints))
    
    with col_chart:
        # è®¡ç®—æ¯ä¸ªé—®é¢˜çš„ç™¾åˆ†æ¯”ï¼ˆåŸºäºæ€»è´Ÿé¢è¯„è®ºæ•°ï¼Œè€Œä¸æ˜¯å»é‡åçš„æ•°é‡ï¼‰
        complaint_counts_with_pct = complaint_counts.copy()
        complaint_counts_with_pct['ç™¾åˆ†æ¯”'] = (complaint_counts_with_pct['å‡ºç°æ¬¡æ•°'] / total_negative_count * 100).round(1)
        
        # è®¡ç®—æ¯ä¸ªé—®é¢˜çš„ç™¾åˆ†æ¯”ï¼ˆåŸºäºæ€»è´Ÿé¢è¯„è®ºæ•°ï¼‰
        custom_percentages = []
        for idx, row in complaint_counts_with_pct.iterrows():
            pct = row['ç™¾åˆ†æ¯”']
            custom_percentages.append(pct)
        
        # åˆ›å»ºå¯äº¤äº’çš„é¥¼å›¾
        # ä½¿ç”¨ texttemplate æ¥æ˜¾ç¤ºæ ‡ç­¾å’ŒåŸºäºæ€»è´Ÿé¢è¯„è®ºæ•°çš„ç™¾åˆ†æ¯”
        fig = go.Figure(data=[go.Pie(
            labels=complaint_counts['é—®é¢˜ç±»å‹'],
            values=complaint_counts['å‡ºç°æ¬¡æ•°'],
            hole=0.4,  # ç”œç”œåœˆæ ·å¼
            marker=dict(
                colors=colors,
                line=dict(color='#ffffff', width=2)
            ),
            texttemplate='%{label}<br>%{text}',  # è‡ªå®šä¹‰æ–‡æœ¬æ¨¡æ¿ï¼šæ˜¾ç¤ºæ ‡ç­¾å’Œç™¾åˆ†æ¯”
            text=[f"{pct:.1f}%" for pct in custom_percentages],  # æ˜¾ç¤ºåŸºäºæ€»è´Ÿé¢è¯„è®ºæ•°çš„ç™¾åˆ†æ¯”
            textposition='outside',
            textfont=dict(size=12),
            hovertemplate="<b>%{label}</b><br>å‡ºç°æ¬¡æ•°: %{value}<br>å æ¯”: %{customdata:.1f}%<extra></extra>",
            customdata=custom_percentages,  # ä¼ é€’ç™¾åˆ†æ¯”æ•°æ®ç”¨äº hover
            pull=[0.05 if i == 0 else 0 for i in range(n_issues)]  # çªå‡ºæœ€ä¸¥é‡çš„é—®é¢˜
        )])
        
        fig.update_layout(
            showlegend=False,
            margin=dict(t=20, b=20, l=20, r=20),
            height=300,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            annotations=[
                dict(
                    text=f"<b>{len(sorted_complaints)}</b><br>ç±»è´Ÿé¢è¯„è®º",
                    x=0.5, y=0.5,
                    font=dict(size=14, color='#374151'),
                    showarrow=False
                )
            ]
        )
        
        # æ˜¾ç¤ºå›¾è¡¨å¹¶æ•è·ç‚¹å‡»äº‹ä»¶
        selected_point = st.plotly_chart(
            fig, 
            use_container_width=True, 
            key="complaint_pie_chart",
            on_select="rerun",
            selection_mode="points"
        )
        
        # å¤„ç†ç‚¹å‡»äº‹ä»¶
        if selected_point and selected_point.selection and selected_point.selection.point_indices:
            clicked_idx = selected_point.selection.point_indices[0]
            clicked_complaint = complaint_counts.iloc[clicked_idx]['é—®é¢˜ç±»å‹']
            if st.session_state.get('selected_complaint_filter') != clicked_complaint:
                st.session_state['selected_complaint_filter'] = clicked_complaint
                st.rerun()
    
    with col_insight:
        st.markdown("**ğŸ’¡ å…³é”®æ´å¯Ÿ**")
        top_issue = sorted_complaints[0]['complaint']
        top_count = sorted_complaints[0]['count']
        
        # ä¸¥é‡ç¨‹åº¦æŒ‡ç¤ºå™¨ - ä½¿ç”¨æ€»è´Ÿé¢è¯„è®ºæ•°
        total_negative_count = st.session_state.get('total_negative_reviews', sum(agg['count'] for agg in sorted_complaints))
        severity_pct = top_count / total_negative_count * 100 if total_negative_count > 0 else 0
        if severity_pct >= 50:
            severity_label = "ğŸ”´ é«˜åº¦é›†ä¸­"
            severity_color = "#dc2626"
        elif severity_pct >= 30:
            severity_label = "ğŸŸ¡ ä¸­åº¦é›†ä¸­"
            severity_color = "#f59e0b"
        else:
            severity_label = "ğŸŸ¢ åˆ†æ•£"
            severity_color = "#10b981"
        
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, #fef2f2 0%, #fff7ed 100%); 
                    padding: 1rem; border-radius: 10px; border-left: 4px solid {severity_color};">
            <p style="margin: 0 0 0.5rem 0; color: #6b7280; font-size: 0.85rem;">æœ€çªå‡ºé—®é¢˜</p>
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #1f2937;">{top_issue}</p>
            <p style="margin: 0; color: #374151;">
                å‡ºç° <strong>{top_count}</strong> æ¬¡ Â· å æ¯” <strong>{severity_pct:.0f}%</strong>
            </p>
            <p style="margin: 0.5rem 0 0 0; font-size: 0.85rem;">{severity_label}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # æ˜¾ç¤ºå…¶ä»–é—®é¢˜çš„ç®€è¦ç»Ÿè®¡
        if len(sorted_complaints) > 1:
            st.markdown("**ğŸ“‹ å…¶ä»–é—®é¢˜**")
            total_negative_count = st.session_state.get('total_negative_reviews', sum(agg['count'] for agg in sorted_complaints))
            for agg in sorted_complaints[1:]:
                pct = agg['count'] / total_negative_count * 100 if total_negative_count > 0 else 0
                st.markdown(f"- {agg['complaint']}: **{agg['count']}** æ¬¡ ({pct:.0f}%)")
    
    # è¿‡æ»¤æ§åˆ¶
    current_filter = st.session_state.get('selected_complaint_filter')
    
    if current_filter:
        st.info(f"ğŸ” å½“å‰è¿‡æ»¤ï¼š**{current_filter}** Â· [ç‚¹å‡»æ¸…é™¤è¿‡æ»¤]")
        if st.button("âœ–ï¸ æ¸…é™¤è¿‡æ»¤ï¼Œæ˜¾ç¤ºå…¨éƒ¨", key="clear_filter"):
            st.session_state['selected_complaint_filter'] = None
            st.rerun()
    
    # æ˜¾ç¤ºåˆ†æç»“æœ - ä½¿ç”¨å¡ç‰‡å¼å¸ƒå±€ï¼Œæ”¯æŒè¿‡æ»¤
    st.markdown("### ğŸ“‹ å½’å› åˆ†æç»“æœ")
    
    # æ ¹æ®è¿‡æ»¤å™¨ç­›é€‰è¦æ˜¾ç¤ºçš„é—®é¢˜
    display_complaints = aggregated_complaints
    if current_filter:
        display_complaints = [agg for agg in aggregated_complaints if agg['complaint'] == current_filter]
        st.caption(f"å·²è¿‡æ»¤æ˜¾ç¤º **{len(display_complaints)}** ç±»é—®é¢˜")
    else:
        # ä½¿ç”¨å®é™…çš„è´Ÿé¢è¯„è®ºæ€»æ•°
        total_review_count = st.session_state.get('total_negative_reviews', sum(agg['count'] for agg in aggregated_complaints))
        st.caption(f"å…±è¯†åˆ«å‡º **{len(aggregated_complaints)}** ç±»é—®é¢˜ï¼Œæ¶‰åŠ **{total_review_count}** æ¡è´Ÿé¢è¯„ä»·")
    
    # è·å– RAG ç»„ä»¶ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
    vectorstore = st.session_state.get('vectorstore', None)
    llm = st.session_state.get('llm', None)
    qa_chain = {'vectorstore': vectorstore, 'llm': llm} if vectorstore and llm else None
    
    for idx, agg in enumerate(display_complaints):
        # ä½¿ç”¨ RAG è¿›è¡ŒçœŸå®åˆ†æï¼ˆä»å‘é‡åº“ä¸­æ£€ç´¢ï¼‰
        # ä¼˜å…ˆä½¿ç”¨ summaryï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ complaint
        query_text = agg.get('summary', agg['complaint'])
        if not query_text:
            query_text = agg['complaint']
        
        spec_match, conclusion, source_docs = match_with_spec(
            query_text, 
            qa_chain=qa_chain
        )
        
        # æå–ç»“è®ºçš„ç®€çŸ­ç‰ˆæœ¬ç”¨äºæ ‡é¢˜
        conclusion_short = conclusion.split(' - ')[0] if ' - ' in conclusion else (conclusion[:30] + "..." if len(conclusion) > 30 else conclusion)
        
        # ä½¿ç”¨ expander å±•ç¤ºæ¯ä¸ªé—®é¢˜ç±»å‹çš„è¯¦æƒ…ï¼ˆé»˜è®¤å±•å¼€å‰3ä¸ªæˆ–è¿‡æ»¤åçš„å…¨éƒ¨ï¼‰
        with st.expander(
            f"**{agg['complaint']}** Â· å‡ºç° {agg['count']} æ¬¡ Â· {conclusion_short}",
            expanded=(idx < 3 or current_filter is not None)  # é»˜è®¤å±•å¼€å‰3ä¸ªï¼Œè¿‡æ»¤åå…¨éƒ¨å±•å¼€
        ):
            col_left, col_right = st.columns([1, 1])
            
            with col_left:
                st.markdown("##### ğŸ—£ï¸ ç”¨æˆ·æŠ±æ€¨ç‚¹")
                st.markdown(f"**{agg['complaint']}**")
                st.markdown(f"ğŸ“Š å‡ºç°æ¬¡æ•°ï¼š**{agg['count']}** æ¬¡")
                
                # å¦‚æœæœ‰ summaryï¼Œæ˜¾ç¤º AI ç”Ÿæˆçš„æ‘˜è¦
                if agg.get('summary'):
                    st.markdown("##### ğŸ¤– AI æ‘˜è¦")
                    st.info(agg['summary'])
                
                st.markdown("##### ğŸ’¬ å…¸å‹ç”¨æˆ·åé¦ˆ")
                # æ˜¾ç¤ºæ‰€æœ‰è¯„è®ºï¼Œä½¿ç”¨å¯æ»šåŠ¨çš„æ–¹å¼
                if len(agg['reviews']) <= 5:
                    # å¦‚æœè¯„è®ºä¸å¤šï¼Œå…¨éƒ¨æ˜¾ç¤º
                    for i, review in enumerate(agg['reviews'], 1):
                        st.markdown(f"**åé¦ˆ {i}:**")
                        st.markdown(f"> *\"{review}\"*")
                        if i < len(agg['reviews']):
                            st.markdown("")  # æ·»åŠ é—´è·
                else:
                    # å¦‚æœè¯„è®ºè¾ƒå¤šï¼Œæ˜¾ç¤ºå‰5æ¡ï¼Œå…¶ä½™åœ¨expanderä¸­
                    for i, review in enumerate(agg['reviews'][:5], 1):
                        st.markdown(f"**åé¦ˆ {i}:**")
                        st.markdown(f"> *\"{review}\"*")
                        if i < 5:
                            st.markdown("")  # æ·»åŠ é—´è·
                    
                    with st.expander(f"ğŸ“‹ æŸ¥çœ‹å…¨éƒ¨ {len(agg['reviews'])} æ¡åé¦ˆ", expanded=False):
                        for i, review in enumerate(agg['reviews'][5:], 6):
                            st.markdown(f"**åé¦ˆ {i}:**")
                            st.markdown(f"> *\"{review}\"*")
                            if i < len(agg['reviews']):
                                st.markdown("")
            
            with col_right:
                st.markdown("##### ğŸ“– è¯´æ˜ä¹¦å¯¹åº”å‚æ•°")
                # ä½¿ç”¨ text_area æˆ– markdown æ¥æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œè€Œä¸æ˜¯ st.infoï¼ˆå¯èƒ½æˆªæ–­ï¼‰
                if len(spec_match) > 500:
                    # å¦‚æœå†…å®¹å¾ˆé•¿ï¼Œä½¿ç”¨ expander æˆ– text_area
                    with st.expander("ğŸ“„ æŸ¥çœ‹å®Œæ•´è¯´æ˜ä¹¦å†…å®¹", expanded=True):
                        st.markdown(spec_match)
                    st.caption("ğŸ’¡ ç‚¹å‡»ä¸Šæ–¹å±•å¼€æŸ¥çœ‹å®Œæ•´å†…å®¹")
                else:
                    # å†…å®¹è¾ƒçŸ­ï¼Œç›´æ¥æ˜¾ç¤º
                    st.markdown(f"<div style='background-color: #f0f9ff; padding: 1rem; border-radius: 8px; border-left: 4px solid #0ea5e9;'>{spec_match}</div>", unsafe_allow_html=True)
                
                # å¦‚æœæœ‰æºæ–‡æ¡£ï¼Œæ˜¾ç¤ºè¯æ®æ¥æºï¼ˆæ˜¾ç¤ºæ‰€æœ‰ç›¸å…³è¯æ®ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
                if source_docs:
                    st.markdown("")
                    with st.expander(f"ğŸ“š æ£€ç´¢åˆ°çš„è¯æ®æ¥æº ({len(source_docs)} æ¡)", expanded=False):
                        for i, doc in enumerate(source_docs, 1):
                            st.markdown(f"**è¯æ® {i}:**")
                            # ä½¿ç”¨ text_area æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œæ”¯æŒæ»šåŠ¨
                            st.text_area(
                                label="",
                                value=doc,
                                height=150,
                                key=f"source_doc_{idx}_{i}",
                                disabled=True,
                                label_visibility="collapsed"
                            )
                            if i < len(source_docs):
                                st.markdown("---")
                
                st.markdown("##### ğŸ¤– AI åˆ¤å®šç»“è®º")
                # ç¡®ä¿ç»“è®ºå®Œæ•´æ˜¾ç¤º
                if "âœ…" in conclusion:
                    st.success(conclusion)
                elif "âš ï¸" in conclusion:
                    st.warning(conclusion)
                elif "â“" in conclusion:
                    st.info(conclusion)
                else:
                    st.info(conclusion)

st.markdown("---")

# ==================== åº•éƒ¨ Action åŒº ====================
st.markdown("## ğŸ¯ è¡ŒåŠ¨å»ºè®®")
st.caption("åŸºäº RAG åˆ†æç»“æœåŠ¨æ€ç”Ÿæˆçš„å¯æ‰§è¡Œè¡ŒåŠ¨é¡¹ Â· ç‚¹å‡»æŒ‰é’®ç«‹å³æ‰§è¡Œ")

if 'aggregated_complaints' in st.session_state and 'llm' in st.session_state:
    aggregated_complaints = st.session_state['aggregated_complaints']
    llm = st.session_state['llm']
    vectorstore = st.session_state.get('vectorstore', None)
    qa_chain = {'vectorstore': vectorstore, 'llm': llm} if vectorstore and llm else None
    
    # ä¸ºæ¯ä¸ªé—®é¢˜èšç±»ç”Ÿæˆ Action Plan
    if 'action_plans' not in st.session_state:
        st.session_state['action_plans'] = {}
    
    # å…ˆä¸ºæ‰€æœ‰é—®é¢˜ç”Ÿæˆ Action Planï¼ˆå¦‚æœè¿˜æ²¡æœ‰ç”Ÿæˆï¼‰
    if 'action_plans_generated' not in st.session_state:
        st.session_state['action_plans_generated'] = True
        with st.spinner("ğŸ¤– æ­£åœ¨ä¸ºæ‰€æœ‰é—®é¢˜ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’..."):
            for idx, agg in enumerate(aggregated_complaints):
                topic_name = agg['complaint']
                action_key = f"action_plan_{idx}"
                
                if action_key not in st.session_state['action_plans']:
                    # è·å– RAG ç»“è®º
                    query_text = agg.get('summary', topic_name)
                    spec_match, conclusion, source_docs = match_with_spec(query_text, qa_chain=qa_chain)
                    
                    # ç”Ÿæˆ Action Plan
                    action_plan = generate_action_plan(
                        topic_name=topic_name,
                        rag_conclusion=conclusion,
                        user_complaints=agg.get('reviews', [])[:5],
                        llm=llm
                    )
                    
                    if action_plan:
                        st.session_state['action_plans'][action_key] = action_plan
    
    # æ”¶é›†æ‰€æœ‰å·²ç”Ÿæˆçš„ Action Plansï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
    action_plans_with_complaints = []
    for idx, agg in enumerate(aggregated_complaints):
        action_key = f"action_plan_{idx}"
        action_plan = st.session_state['action_plans'].get(action_key)
        if action_plan:
            priority = action_plan.get('priority', 'Medium')
            # ä¼˜å…ˆçº§æ˜ å°„ï¼šHigh=3, Medium=2, Low=1
            priority_score = {'High': 3, 'Medium': 2, 'Low': 1}.get(priority, 2)
            action_plans_with_complaints.append({
                'complaint': agg,
                'action_plan': action_plan,
                'priority_score': priority_score,
                'action_key': action_key
            })
    
    # æŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½æ’åºï¼ˆHigh > Medium > Lowï¼‰ï¼Œç›¸åŒä¼˜å…ˆçº§æŒ‰å‡ºç°æ¬¡æ•°æ’åº
    action_plans_with_complaints.sort(
        key=lambda x: (x['priority_score'], x['complaint']['count']), 
        reverse=True
    )
    
    # æ˜¾ç¤ºå‰ 5 ä¸ª
    top_actions = action_plans_with_complaints[:5]
    
    for item in top_actions:
        agg = item['complaint']
        action_plan = item['action_plan']
        action_key = item['action_key']
        topic_name = agg['complaint']
        
        # ç¡®å®šä¼˜å…ˆçº§æ ·å¼
        priority = action_plan.get('priority', 'Medium')
        if priority == 'High':
            badge_class = "high"
            badge_text = "é«˜ä¼˜å…ˆçº§"
            badge_icon = "ğŸ”´"
            badge_color = "#dc2626"
            badge_bg = "#fef2f2"
        elif priority == 'Low':
            badge_class = "low"
            badge_text = "ä½ä¼˜å…ˆçº§"
            badge_icon = "ğŸŸ¢"
            badge_color = "#059669"
            badge_bg = "#ecfdf5"
        else:
            badge_class = "medium"
            badge_text = "ä¸­ä¼˜å…ˆçº§"
            badge_icon = "ğŸŸ¡"
            badge_color = "#d97706"
            badge_bg = "#fffbeb"
        
        # ä½¿ç”¨ container åˆ›å»ºå¡ç‰‡
        with st.container():
            # å¡ç‰‡å¤´éƒ¨ï¼šä¼˜å…ˆçº§æ ‡ç­¾å’Œé—®é¢˜ç±»å‹
            col_badge, col_topic = st.columns([1, 3])
            with col_badge:
                st.markdown(f'<span style="background:{badge_bg};color:{badge_color};padding:4px 12px;border-radius:20px;font-size:0.8rem;font-weight:600;">{badge_icon} {badge_text}</span>', unsafe_allow_html=True)
            with col_topic:
                st.caption(f"ğŸ“Œ é—®é¢˜ç±»å‹ï¼š{topic_name} Â· æ¶‰åŠ {agg['count']} æ¡åé¦ˆ")
            
            # å¡ç‰‡æ ‡é¢˜
            st.markdown(f"#### {action_plan.get('title', 'è¡ŒåŠ¨è®¡åˆ’')}")
            
            # å¡ç‰‡å†…å®¹åŒº
            col_content, col_action = st.columns([3, 1])
            
            with col_content:
                # æ˜¾ç¤º Action Type
                action_type = action_plan.get('action_type', 'Doc Update')
                type_icons = {
                    'Jira Ticket': 'ğŸ',
                    'Doc Update': 'ğŸ“',
                    'Email Draft': 'ğŸ“§',
                    'Meeting': 'ğŸ“…'
                }
                type_icon = type_icons.get(action_type, 'ğŸ“‹')
                st.markdown(f"**{type_icon} è¡ŒåŠ¨ç±»å‹ï¼š** {action_type}")
                
                # æ˜¾ç¤ºå†…å®¹ï¼ˆé•¿å†…å®¹åªæ˜¾ç¤ºåœ¨ expander ä¸­ï¼Œä¸æ˜¾ç¤ºé¢„è§ˆï¼‰
                content = action_plan.get('content', '')
                if len(content) > 300:
                    # é•¿å†…å®¹ï¼šåªæ˜¾ç¤º expanderï¼Œä¸æ˜¾ç¤ºé¢„è§ˆ
                    with st.expander("ğŸ“„ æŸ¥çœ‹å®Œæ•´å†…å®¹", expanded=False):
                        st.markdown(f"<div style='background-color: #f9fafb; padding: 1rem; border-radius: 8px; border-left: 4px solid #6366f1;'>{content}</div>", unsafe_allow_html=True)
                else:
                    # çŸ­å†…å®¹ï¼šç›´æ¥æ˜¾ç¤º
                    st.markdown(f"<div style='background-color: #f9fafb; padding: 1rem; border-radius: 8px; border-left: 4px solid #6366f1;'>{content}</div>", unsafe_allow_html=True)
            
            with col_action:
                # æ ¹æ® Action Type æ˜¾ç¤ºä¸åŒçš„æŒ‰é’®
                action_type = action_plan.get('action_type', 'Doc Update')
                button_key = f"btn_{action_key}"
                
                if action_type == 'Jira Ticket':
                    if st.button("ğŸš€ æ¨é€è‡³ Jira", key=button_key, use_container_width=True):
                        # ç«‹å³æ˜¾ç¤º toastï¼Œå‡å°‘å»¶è¿Ÿ
                        import random
                        ticket_id = f"DJI-2025-{random.randint(800, 999)}"
                        st.toast(f"âœ… å·¥å•å·²åˆ›å»ºï¼Ticket ID: {ticket_id}", icon="ğŸ‰")
                        st.session_state[f'{button_key}_triggered'] = True
                        st.rerun()
                        
                elif action_type == 'Doc Update':
                    if st.button("ğŸ“ åˆ›å»º Notion Task", key=button_key, use_container_width=True):
                        st.toast("âœ… Notion ä»»åŠ¡å·²åˆ›å»ºï¼", icon="ğŸ“")
                        st.session_state[f'{button_key}_triggered'] = True
                        st.rerun()
                        
                elif action_type == 'Email Draft':
                    if st.button("ğŸ“§ å¤åˆ¶é‚®ä»¶", key=button_key, use_container_width=True):
                        st.toast("âœ… é‚®ä»¶å†…å®¹å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼", icon="ğŸ“§")
                        st.session_state[f'{button_key}_triggered'] = True
                        st.rerun()
                        
                elif action_type == 'Meeting':
                    if st.button("ğŸ“… åˆ›å»ºä¼šè®®", key=button_key, use_container_width=True):
                        st.toast("âœ… ä¼šè®®é‚€è¯·å·²å‘é€ï¼", icon="ğŸ“…")
                        st.session_state[f'{button_key}_triggered'] = True
                        st.rerun()
                
                # æ˜¾ç¤ºè§¦å‘åçš„è¯¦ç»†ä¿¡æ¯
                if st.session_state.get(f'{button_key}_triggered', False):
                    st.markdown("")
                    if action_type == 'Jira Ticket':
                        st.success(f"âœ… å·¥å•å·²æˆåŠŸåˆ›å»ºå¹¶æŒ‡æ´¾ç»™ç›¸å…³å›¢é˜Ÿ")
                        with st.expander("ğŸ å·¥å•è¯¦æƒ…", expanded=True):
                            st.markdown(f"""
| å­—æ®µ | å€¼ |
|------|-----|
| **å·¥å•æ ‡é¢˜** | {action_plan.get('title', 'N/A')} |
| **ç±»å‹** | Bug / åŠŸèƒ½å¢å¼º |
| **ä¼˜å…ˆçº§** | {priority} |
| **æè¿°** | {content[:200]}... |
                            """)
                    elif action_type == 'Email Draft':
                        st.success("âœ… é‚®ä»¶å†…å®¹å·²å¤åˆ¶åˆ°å‰ªè´´æ¿")
                        with st.expander("ğŸ“§ é‚®ä»¶å†…å®¹é¢„è§ˆ", expanded=True):
                            st.markdown(content)
                    elif action_type == 'Meeting':
                        st.success("âœ… ä¼šè®®é‚€è¯·å·²å‘é€")
                        with st.expander("ğŸ“… ä¼šè®®è¯¦æƒ…", expanded=True):
                            st.markdown(content)
            
            st.divider()

elif 'aggregated_complaints' in st.session_state:
    st.info("ğŸ‘† è¯·å…ˆç‚¹å‡»ä¸Šæ–¹ã€Œå¼€å§‹å½’å› åˆ†æã€æŒ‰é’®ï¼ŒAI å°†åŸºäºåˆ†æç»“æœç”Ÿæˆé’ˆå¯¹æ€§çš„è¡ŒåŠ¨å»ºè®®ã€‚")
else:
    st.info("ğŸ‘† è¯·å…ˆç‚¹å‡»ä¸Šæ–¹ã€Œå¼€å§‹å½’å› åˆ†æã€æŒ‰é’®ï¼ŒAI å°†åŸºäºåˆ†æç»“æœç”Ÿæˆé’ˆå¯¹æ€§çš„è¡ŒåŠ¨å»ºè®®ã€‚")

# ==================== é¡µè„š ====================
st.markdown("---")
st.markdown(
    """
    <div style="text-align: center; color: #6b7280; font-size: 0.85rem;">
        <p>ğŸ”¬ ReviewOps v1.0 Â· ç”¨æˆ·åé¦ˆå†³ç­–ä¸­å°</p>
        <p>Powered by RAG + LLM Â· Built with Streamlit</p>
    </div>
    """,
    unsafe_allow_html=True
)

